{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3398b-028f-4299-92e8-d4ff4292ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18dd8cd-43ef-4c3c-bd8b-b56a35c83763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24eb905-116b-482d-bfbe-795dec4cc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFICES = ['U.S. House', 'State House']\n",
    "YEARS = ['2018', '2020', '2022', '2024']\n",
    "\n",
    "# OFFICES = ['U.S. Senate']\n",
    "# YEARS = ['2020']\n",
    "\n",
    "# OFFICES = ['State Senate']\n",
    "# YEARS = ['2022']\n",
    "\n",
    "# OFFICES = ['President']\n",
    "# YEARS = ['2024']\n",
    "\n",
    "\n",
    "\n",
    "# TARGET = 'rep_share' # Good for rep-leaning regions\n",
    "# TARGET = 'dem_share' # Good for dem-leaning regions\n",
    "TARGET = 'partisan_temp' # Good!\n",
    "# TARGET = 'partisanship_lean_change_amount_curr'\n",
    "# TARGET = 'pedersen_index_percent'\n",
    "\n",
    "TOP_N_FEATURES = 100\n",
    "TOP_N_FEATURES_TO_DISPLAY = 15\n",
    "\n",
    "# Socioeconomic data as features in addition\n",
    "# to the original and the engineered features.\n",
    "census_datasets = [\n",
    "    'b02001_race', 'b04007_ancestry', 'b05012_nativity_us', 'b08303_travel_time_work', 'b25003_housing_rentership', \n",
    "    'dp02_selected_social_characteristics', 'dp03_selected_economic_characteristics', 'dp04_housing_characteristics', 'dp05_age_race', \n",
    "    's0101_age_sex', 's1101_households_families', 's1201_marital_status', 's1501_educational_attainment', 's1701_income_poverty', \n",
    "    's1903_median_income', 's2101_veteran_status', 's2201_food_stamps', 's2301_employment_status', 's2401_occupation_sex', \n",
    "    's2403_industry_sex', 's2501_occupancy_characteristics', 's2701_health_insurance', 's2503_financial_characteristics',\n",
    "]\n",
    "\n",
    "# These key-like columns just add noise.\n",
    "drop_features_required = [\n",
    "    'standardized_id', 'standardized_id_num',\n",
    "    'aland_tract', 'awater_tract', 'geoid_tract', 'geoidfq_tract', \n",
    "    'geometry', 'geometry_tract', 'name_tract', 'tractce_tract',\n",
    "    'nearest_bound_census_tract', 'nearest_bound_school_district', 'nearest_bound_zipcode',\n",
    "]\n",
    "\n",
    "# Optionally drop one or more of these during \n",
    "# train/test/prediction.\n",
    "drop_features_optional = [\n",
    "    # 'office_code', \n",
    "    # 'dem_share_prev', \n",
    "    # 'rep_share_prev', 'oth_share_prev', \n",
    "    # 'dem_share_change_prev', 'rep_share_change_prev', 'oth_share_change_prev', \n",
    "    # 'dem_votes_change_prev', 'rep_votes_change_prev', 'oth_votes_change_prev', \n",
    "    # 'registered_voters_change_prev', 'turnout_pct_change_prev', \n",
    "    # 'partisan_temp_prev', 'partisan_temp_change_prev', \n",
    "    # 'partisanship_lean_prev', 'partisanship_lean_change_prev', 'partisanship_lean_change_amount_prev',\n",
    "]\n",
    "\n",
    "# Seen features that may or may not be used as\n",
    "# targets as well.\n",
    "drop_features_seen = [\n",
    "    'dem_votes', 'oth_votes', 'rep_votes', 'total_votes', \n",
    "    'dem_share', 'rep_share', 'oth_share',  'turnout_pct',\n",
    "    'dem_share_change_curr','rep_share_change_curr', 'oth_share_change_curr', \n",
    "    'dem_votes_change_curr','rep_votes_change_curr', 'oth_votes_change_curr', \n",
    "    'partisan_temp', 'partisanship_lean_curr', 'registered_voters',\n",
    "    'registered_voters_change_curr','turnout_pct_change_curr',\n",
    "    'partisan_temp_category', 'partisan_temp_change_curr',\n",
    "    'pedersen_index_percent', 'pedersen_index',\n",
    "    'partisanship_lean_change_amount_curr',\n",
    "]\n",
    "\n",
    "# DO NOT EDIT BELOW THIS LINE\n",
    "if TARGET in drop_features_seen:\n",
    "    drop_features_seen.remove(TARGET) # Keep target in features for later extraction\n",
    "\n",
    "drop_features = drop_features_required + drop_features_optional + drop_features_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d37a04a-8a5a-4aac-9255-1616addf0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Remove top features not shared between \n",
    "    different datasets to prevent errors.'''\n",
    "def removeUncommonColumns(nested_dict):\n",
    "    print(\"Removing uncommon columns...\")\n",
    "    \n",
    "    # Flatten and find common columns\n",
    "    all_dfs = [df for year in nested_dict for df in nested_dict[year].values()]\n",
    "    common_cols = set(all_dfs[0].columns)\n",
    "    for df in all_dfs[1:]:\n",
    "        common_cols &= set(df.columns)\n",
    "    \n",
    "    # Safely trim all dataframes\n",
    "    for year in nested_dict:\n",
    "        for office in nested_dict[year]:\n",
    "            df = nested_dict[year][office]\n",
    "            existing_cols = [col for col in common_cols if col in df.columns]\n",
    "            nested_dict[year][office] = df[existing_cols]\n",
    "\n",
    "    print('Done.')\n",
    "    \n",
    "    return nested_dict\n",
    "\n",
    "\n",
    "''' Pull the engineered feature data along with its\n",
    "    target for each year and office.'''\n",
    "def makeDatasets(years, offices):\n",
    "    print('Making datasets...')\n",
    "    \n",
    "    df_datasets = {}\n",
    "    \n",
    "    for year in years:\n",
    "        print(f'Processing year {year}...')\n",
    "        df_datasets[year] = {}\n",
    "        \n",
    "        for office in offices:\n",
    "            office = office.replace(' ', '_').replace('.', '')\n",
    "            print(f'Processing office {office}...')\n",
    "\n",
    "            df = pd.read_csv('data/generated_data/07_ml_features_' + year + '_' + office + '.csv', low_memory=False)\n",
    "            df_datasets[year][office] = df\n",
    "    \n",
    "    df_datasets = removeUncommonColumns(df_datasets)\n",
    "    print('Done.')\n",
    "    \n",
    "    return df_datasets\n",
    "\n",
    "\n",
    "''' Split features and target into X\n",
    "    and y variables with some cleanup.'''\n",
    "def makeFeaturesTargets(df):\n",
    "    print(f'Making features and target...')\n",
    "    \n",
    "    y = df[[TARGET]]\n",
    "    X = df.drop(columns=['standardized_id_num', 'partisan_temp', 'partisan_temp_change_curr'])\n",
    "    X = X.replace(['-', '(X)', 'N/A', 'null', ''], pd.NA)\n",
    "    \n",
    "    X, y = X.align(y.dropna(), join='inner', axis=0)\n",
    "    \n",
    "    print('Done.')\n",
    "    return X, y\n",
    "\n",
    "\n",
    "''' Pipeline to impute and encode categorical\n",
    "    variables, as well as scale, etc.'''\n",
    "def fitModel(X, y, k=5):\n",
    "    print(f'Fitting model...')\n",
    "\n",
    "    # Define non-numeric features\n",
    "    categorical_cols = [\n",
    "        'office_code',\n",
    "        'partisanship_lean_curr',\n",
    "        'partisanship_lean_prev',\n",
    "        'partisanship_lean_change_prev',\n",
    "    ]\n",
    "\n",
    "    # Format the columns\n",
    "    categorical_cols = [col for col in categorical_cols if col in X.columns]\n",
    "    numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col not in categorical_cols]\n",
    "\n",
    "    # Set up the pipeline\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "    ])\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        ('num', numeric_transformer, numeric_cols)\n",
    "    ])\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    # K-fold CV\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2', n_jobs=-1)\n",
    "    mse_scores = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "    print(f'Average R¬≤ across {k} folds: {r2_scores.mean():.4f} ¬± {r2_scores.std():.4f}')\n",
    "    print(f'Average MSE across {k} folds: {mse_scores.mean():.4f} ¬± {mse_scores.std():.4f}')\n",
    "\n",
    "    # Final fit\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print('Final model fitted on training split.')\n",
    "    return model, X_train, X_test, y_train, y_test, numeric_cols\n",
    "\n",
    "\n",
    "def makePredictions(X_test, model):\n",
    "    print(f'Making predictions...')\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Done.')\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def plotAccuracy(y_test, y_pred):\n",
    "    print(f'Plotting accuracy...')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.4)\n",
    "    plt.xlim(-1.25, 1.25)\n",
    "    plt.ylim(-1.25, 1.25)\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(\"Prediction Accuracy\")\n",
    "    plt.grid(True)\n",
    "    print('Done.')\n",
    "    return plt\n",
    "\n",
    "\n",
    "def featureCoeff(model):\n",
    "    print(f'Computing feature coefficients from pipeline...')\n",
    "\n",
    "    regressor = model.named_steps['regressor']\n",
    "    preprocessor = model.named_steps['preprocessor']\n",
    "    coef = regressor.coef_.flatten()\n",
    "\n",
    "    # Inline feature name extraction\n",
    "    output_features = []\n",
    "    for name, transformer, columns in preprocessor.transformers_:\n",
    "        if transformer == 'drop' or transformer is None:\n",
    "            continue\n",
    "        if hasattr(transformer, 'get_feature_names_out'):\n",
    "            try:\n",
    "                names = transformer.get_feature_names_out(columns)\n",
    "            except:\n",
    "                names = columns\n",
    "        else:\n",
    "            names = columns\n",
    "        output_features.extend(names)\n",
    "\n",
    "    feature_names = output_features\n",
    "\n",
    "    if len(coef) != len(feature_names):\n",
    "        raise ValueError(f\"Mismatch: {len(coef)} coefficients vs {len(feature_names)} feature names\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coef,\n",
    "        'abs_coefficient': np.abs(coef)\n",
    "    })\n",
    "\n",
    "    top_features = df.sort_values(by='abs_coefficient', ascending=False).head(TOP_N_FEATURES_TO_DISPLAY)\n",
    "    print('Done.')\n",
    "    return top_features\n",
    "\n",
    "\n",
    "def plotFeatureCoeff(features):\n",
    "    print(f'Plotting feature coefficients...')\n",
    "    plt.figure(figsize=(12, 18))\n",
    "    bars = plt.barh(features['feature'], features['coefficient'])\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.xticks(fontsize=7)\n",
    "    plt.yticks(fontsize=7)\n",
    "    plt.title(f'Most Influential Features (Linear Regression)')\n",
    "    plt.axvline(x=0, color='gray', linestyle='--')\n",
    "    plt.grid(True, axis='x', linestyle=':', alpha=0.7)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    print('Done.')\n",
    "    return plt\n",
    "\n",
    "\n",
    "def get_feature_names(model):\n",
    "    print(\"Getting feature names...\")\n",
    "\n",
    "    preprocessor = model.named_steps['preprocessor']\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "    # Store both prefixed and cleaned names\n",
    "    cleaned_names = [name.split('__', 1)[-1] for name in feature_names]\n",
    "\n",
    "    print(f\"Done. Retrieved {len(feature_names)} feature names.\")\n",
    "    return feature_names, cleaned_names\n",
    "\n",
    "\n",
    "def mergeTopFeatures(top_features_lists):\n",
    "    print(f'Creating common top features using clean names...')\n",
    "    from itertools import chain\n",
    "\n",
    "    # Make clean names\n",
    "    normalized_lists = []\n",
    "    for item in top_features_lists:\n",
    "        if isinstance(item, list):\n",
    "            normalized_lists.append(item)\n",
    "        elif hasattr(item, 'columns') and 'feature' in item.columns:\n",
    "            normalized_lists.append(item['feature'].tolist())\n",
    "        else:\n",
    "            raise ValueError(\"Each item must be a list or a DataFrame with a 'feature' column\")\n",
    "\n",
    "    # Find intersection\n",
    "    common_features = set(normalized_lists[0])\n",
    "    for feature_list in normalized_lists[1:]:\n",
    "        common_features.intersection_update(feature_list)\n",
    "\n",
    "    # Preserve order\n",
    "    seen = set()\n",
    "    merged_common_ordered = []\n",
    "\n",
    "    for item in chain.from_iterable(normalized_lists):\n",
    "        if item in common_features and item not in seen:\n",
    "            seen.add(item)\n",
    "            merged_common_ordered.append(item)\n",
    "\n",
    "    print('Done.')\n",
    "    return merged_common_ordered\n",
    "\n",
    "\n",
    "def one_hot_encode_selected(df, columns_to_encode):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if not columns_to_encode:\n",
    "        return df\n",
    "        \n",
    "    encoded = pd.get_dummies(df[columns_to_encode], prefix=columns_to_encode)\n",
    "    df = df.drop(columns=columns_to_encode)\n",
    "    \n",
    "    return pd.concat([df, encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430706f-0828-4013-91a3-195f2662f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRankedFeatureList(target=TARGET):\n",
    "    df_ranked_features = pd.read_csv(f'data/generated_data/df_importances_{target}.csv')\n",
    "    df_ranked_features = df_ranked_features.sort_values(by='Average', ascending=False)\n",
    "    df_ranked_features = df_ranked_features[~df_ranked_features['Feature name'].isin(drop_features)]\n",
    "    \n",
    "    df_ranked_features_top = df_ranked_features.head(TOP_N_FEATURES)\n",
    "    \n",
    "    ranked_features_top_list = df_ranked_features_top['Feature name'].tolist()\n",
    "    \n",
    "    return ranked_features_top_list\n",
    "\n",
    "ranked_features_top_list = getRankedFeatureList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490575b3-eb22-42a4-b9a4-91a0a25e290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predDatasetsIndiv(df_datasets, years, offices):\n",
    "    for year in years:\n",
    "        print(f'Processing year {year}...')\n",
    "        \n",
    "        for office in offices:\n",
    "            office = office.replace(' ', '_').replace('.', '')\n",
    "            print(f'Processing office {office}...')\n",
    "            \n",
    "            df = df_datasets[year][office].copy()\n",
    "            X, y = makeFeaturesTargets(df)\n",
    "            X = X[ranked_features_top_list]\n",
    "    \n",
    "            print(f\"Training over {len(X.columns)} features...\")\n",
    "            \n",
    "            model, X_train, X_test, y_train, y_test, numeric_cols = fitModel(X, y)\n",
    "            y_pred = makePredictions(X_test, model)\n",
    "            \n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            print(\"Mean Squared Error:\", mse)\n",
    "    \n",
    "            r2 = model.score(X_test, y_test)\n",
    "            print(\"R2 Score:\", r2)\n",
    "    \n",
    "            plt = plotAccuracy(y_test, y_pred)\n",
    "            plt.savefig(f'output/figures/regression_accuracy_{year}_{office}_individual.png')\n",
    "            # plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            top_features = featureCoeff(model)\n",
    "            plt = plotFeatureCoeff(top_features)\n",
    "            plt.savefig(f'output/figures/regression_top_features_{year}_{office}_individual.png')\n",
    "            # plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513574a-ab5e-4dcd-849c-675203956a52",
   "metadata": {},
   "source": [
    "#### Predict Individual Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb48fc8-b90a-42cf-94ac-e86a61a277bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2018', '2020', '2022']\n",
    "offices = ['U.S. House']\n",
    "\n",
    "df_datasets = makeDatasets(years, offices)\n",
    "predDatasetsIndiv(df_datasets, years, offices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc1e41-2798-4c30-a4c6-0c902b1edcc9",
   "metadata": {},
   "source": [
    "#### Fit/Train Final Model\n",
    "Do we have data leakage here? Maybe not, if we train on historical data and\n",
    "<br>run a separate test on newer data in a following step. Say, here we\n",
    "train 2018-2022, <br>and then in another cell test 2024 on the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff2391-6737-4fd6-bbef-7c016a99101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggDatasets(datasets, years, offices):\n",
    "    dfs = []\n",
    "    \n",
    "    for year in years:\n",
    "        print(f'Processing year {year}...')\n",
    "        for office in offices:\n",
    "            office = office.replace(' ', '_').replace('.', '')\n",
    "            \n",
    "            print(f'Processing office {office}...')\n",
    "            dfs.append(df_datasets[year][office].copy())\n",
    "            \n",
    "    df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af13e0-d993-4e83-9bba-ecac5367c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitDatasetsAgg(df_datasets):\n",
    "    X, y = makeFeaturesTargets(df)\n",
    "    X = X[ranked_features_top_list]\n",
    "\n",
    "    print(f\"Training over {len(X.columns)} features...\")\n",
    "    \n",
    "    model, X_train, X_test, y_train, y_test, numeric_cols = fitModel(X, y)\n",
    "    y_pred = makePredictions(X_test, model)\n",
    "\n",
    "    expected_columns = X.columns.tolist()\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "    r2 = model.score(X_test, y_test)\n",
    "    print(\"R2 Score:\", r2)\n",
    "\n",
    "    plt = plotAccuracy(y_test, y_pred)\n",
    "    plt.savefig(f'output/figures/regression_accuracy_aggregate.png')\n",
    "    plt.show()\n",
    "    # plt.close()\n",
    "    \n",
    "    top_features = featureCoeff(model)\n",
    "    plt = plotFeatureCoeff(top_features)\n",
    "    plt.savefig(f'output/figures/regression_top_features_aggregate.png')\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a38f2ea-b9c2-40a0-a7a7-d24dfc5b769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predDatasetsAgg(df_datasets):\n",
    "    X, y = makeFeaturesTargets(df)\n",
    "    X = X[ranked_features_top_list]\n",
    "\n",
    "    print(f\"Training over {len(X.columns)} features...\")\n",
    "    \n",
    "    model, X_train, X_test, y_train, y_test, numeric_cols = fitModel(X, y)\n",
    "    y_pred = makePredictions(X_test, model)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "    r2 = model.score(X_test, y_test)\n",
    "    print(\"R2 Score:\", r2)\n",
    "\n",
    "    plt = plotAccuracy(y_test, y_pred)\n",
    "    plt.savefig(f'output/figures/regression_accuracy_aggregate.png')\n",
    "    plt.show()\n",
    "    # plt.close()\n",
    "    \n",
    "    top_features = featureCoeff(model)\n",
    "    plt = plotFeatureCoeff(top_features)\n",
    "    plt.savefig(f'output/figures/regression_top_features_aggregate.png')\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bd73f-b048-420e-ae16-dedcb315983f",
   "metadata": {},
   "source": [
    "#### Fit aggregated data\n",
    "This functionality produces the <code>model</code> object to be used later. Make sure no datasets from this\n",
    "<br>cell is NOT included in holdout testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ad1a2a-20aa-4128-9c15-1624cc9720da",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2018', '2020', '2022']\n",
    "offices = ['U.S. House']\n",
    "\n",
    "df_datasets = makeDatasets(years, offices)\n",
    "df = aggDatasets(df_datasets, years, offices)\n",
    "\n",
    "X, y = makeFeaturesTargets(df)\n",
    "X = X[ranked_features_top_list]\n",
    "X = X.replace({pd.NA: np.nan})\n",
    "\n",
    "model, X_train, X_test, y_train, y_test, numeric_cols = fitModel(X, y)\n",
    "y_pred = makePredictions(X_test, model)\n",
    "\n",
    "expected_columns = X.columns.tolist()\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(\"R2 Score:\", r2)\n",
    "\n",
    "plt = plotAccuracy(y_test, y_pred)\n",
    "plt.savefig(f'output/figures/regression_accuracy_aggregate.png')\n",
    "plt.show()\n",
    "\n",
    "top_features = featureCoeff(model)\n",
    "plt = plotFeatureCoeff(top_features)\n",
    "plt.savefig(f'output/figures/regression_top_features_aggregate.png')\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9beb1b-c5a0-4485-bf8c-bc0a243c06f0",
   "metadata": {},
   "source": [
    "#### Holdout Prediction\n",
    "This functionality requires the <code>model</code> object from a previous cell. Make sure this holdout\n",
    "<br>dataset was not included in the model's training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a7b6d-0195-4f39-a941-7bc883c894fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2024']\n",
    "offices = ['U.S. House']\n",
    "\n",
    "df_datasets = makeDatasets(years, offices)\n",
    "df = df_datasets['2024']['US_House']\n",
    "\n",
    "df_orig = df.copy()\n",
    "\n",
    "X, y = makeFeaturesTargets(df)\n",
    "X = X[ranked_features_top_list]\n",
    "\n",
    "# Scale X\n",
    "X = X.replace({pd.NA: np.nan})\n",
    "\n",
    "# NaN missing cells\n",
    "X = X.replace({pd.NA: np.nan})\n",
    "\n",
    "# # Predict, transformations handled by pipeline\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# MSE\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# R2\n",
    "r2 = model.score(X, y)\n",
    "print(\"R2 Score:\", r2)\n",
    "\n",
    "with open(f'output/reports/prediction_eval_{TARGET}_holdout_regression.txt', 'w') as f:\n",
    "    output = f\"R2 Score: {r2}\"\n",
    "    output += f\"\\nMSE: {mse}\"\n",
    "    f.write(output)\n",
    "\n",
    "plt = plotAccuracy(y, y_pred)\n",
    "plt.savefig(f'output/figures/regression_accuracy_{TARGET}_holdout.png')\n",
    "plt.close()\n",
    "# plt.show()\n",
    "\n",
    "top_features = featureCoeff(model)\n",
    "plt = plotFeatureCoeff(top_features)\n",
    "plt.savefig(f'output/figures/regression_top_features_{TARGET}_holdout.png')\n",
    "plt.close()\n",
    "# plt.show()\n",
    "\n",
    "y_pred = model.predict(X).ravel()\n",
    "df_orig[f\"predicted_{TARGET}\"] = pd.Series(y_pred, index=X.index)\n",
    "\n",
    "df_export = df_orig[[\"standardized_id_num\", TARGET, f\"predicted_{TARGET}\"]].copy()\n",
    "df_export[\"standardized_id_num\"] = df_export[\"standardized_id_num\"].apply(lambda x: str(x).replace('.0', '').zfill(13))\n",
    "\n",
    "filename = f\"data/generated_data/predicted_{TARGET}_{years[0]}_{offices[0].replace('.', '').replace(' ', '_')}.csv\"\n",
    "df_export.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"Predictions saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a128ef0-988e-4632-ae09-1594510ec32a",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ad5f8-313f-4da8-a88c-6570cc20fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "def gridSearch(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    X, y = makeFeaturesTargets(df)\n",
    "    X = X[ranked_features_top_list]\n",
    "    X = X.replace({pd.NA: np.nan})\n",
    "    \n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "    \n",
    "    models = {\n",
    "        'LinearRegression': (\n",
    "            LinearRegression(), \n",
    "            {}\n",
    "        ),\n",
    "        'Ridge': (\n",
    "            Ridge(),\n",
    "            {'regressor__alpha': [0.01, 0.1, 1.0, 10]}\n",
    "        ),\n",
    "        'Lasso': (\n",
    "            Lasso(max_iter=10000),\n",
    "            {'regressor__alpha': [0.01, 0.1, 1.0, 10]}\n",
    "        ),\n",
    "        'ElasticNet': (\n",
    "            ElasticNet(max_iter=10000), \n",
    "            {\n",
    "            'regressor__alpha': [0.01, 0.1, 1.0],\n",
    "            'regressor__l1_ratio': [0.2, 0.5, 0.8]\n",
    "            }\n",
    "        ),\n",
    "        'DecisionTree': (\n",
    "            DecisionTreeRegressor(), \n",
    "            {\n",
    "            'regressor__max_depth': [5, 10, None],\n",
    "            'regressor__min_samples_split': [2, 10]\n",
    "            }\n",
    "        ),\n",
    "        'RandomForest': (\n",
    "            RandomForestRegressor(n_jobs=-1), \n",
    "            {\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__max_depth': [5, 10, None]\n",
    "            }\n",
    "        ),\n",
    "        'GradientBoosting': (\n",
    "            GradientBoostingRegressor(), \n",
    "            {\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__learning_rate': [0.05, 0.1],\n",
    "            'regressor__max_depth': [3, 5]\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    y = y.values.ravel() \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    best_models = {}\n",
    "    results = []\n",
    "    \n",
    "    for name, (model, params) in models.items():\n",
    "        print(f\"üîç Tuning: {name}\")\n",
    "        \n",
    "        pipe = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', model)\n",
    "        ])\n",
    "        \n",
    "        grid = GridSearchCV(pipe, param_grid=params, cv=5, scoring='r2', n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "    \n",
    "        best_model = grid.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    \n",
    "        mse = mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Best Params': grid.best_params_,\n",
    "            'MSE': mse,\n",
    "            'R2': r2\n",
    "        })\n",
    "    \n",
    "        best_models[name] = grid \n",
    "    \n",
    "    results_df = pd.DataFrame(results).sort_values(by='R2', ascending=False)\n",
    "\n",
    "    print('Done.')\n",
    "\n",
    "    return best_models, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd77a93-9de0-4a8a-b4ce-b9f2117eb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2018', '2020', '2022']\n",
    "offices = ['U.S. House']\n",
    "\n",
    "df_datasets = makeDatasets(years, offices)\n",
    "df = aggDatasets(df_datasets, years, offices)\n",
    "\n",
    "best_models, results_df = gridSearch(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11416d-42e5-46b4-b137-8b06799277e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitBestModel(df, params):\n",
    "    df = df.copy()\n",
    "    \n",
    "    X, y = makeFeaturesTargets(df)\n",
    "    X = X[ranked_features_top_list]\n",
    "    X = X.replace({pd.NA: np.nan})\n",
    "    \n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "    \n",
    "    gbr_final = GradientBoostingRegressor(\n",
    "        random_state=42,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', gbr_final)\n",
    "    ])\n",
    "    \n",
    "    model.fit(X, y.values.ravel())\n",
    "\n",
    "    print('Done.')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9cbbe-7b78-4c50-b516-9babe9d2fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanParams(best_models, model_name):\n",
    "    params = best_models[model_name].best_params_\n",
    "    clean_params = {k.replace('regressor__', ''): v for k, v in params.items()}\n",
    "    \n",
    "    print(f'Cleaned params for {model_name}: {clean_params}')\n",
    "    \n",
    "    return clean_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e29415-c280-4986-8e51-4627e052f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2018', '2020', '2022']\n",
    "offices = ['U.S. House']\n",
    "\n",
    "df_datasets = makeDatasets(years, offices)\n",
    "df = aggDatasets(df_datasets, years, offices)\n",
    "\n",
    "clean_params = cleanParams(best_models, 'GradientBoosting')\n",
    "model = fitBestModel(df, clean_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011cd4e5-b222-4801-b9eb-22c37334a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePredsBestModel(df, model):\n",
    "    df = df.copy()\n",
    "    \n",
    "    X, y = makeFeaturesTargets(df)\n",
    "    X = X[ranked_features_top_list]\n",
    "    X = X.replace({pd.NA: np.nan})\n",
    "    X = X.dropna()\n",
    "    \n",
    "    # Align datagrame to cleaned X\n",
    "    df = df.loc[X.index]\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    y_true = df[TARGET]\n",
    "    \n",
    "    df[f\"predicted_{TARGET}\"] = y_pred\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "\n",
    "    return df, y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b71a2d-56a4-4fad-810a-242f1593b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2024']\n",
    "offices = ['U.S. House']\n",
    "\n",
    "df_datasets = makeDatasets(years, offices)\n",
    "df_holdout = df_datasets['2024']['US_House']\n",
    "\n",
    "df_pred_best, y_pred_best, y_true_best = makePredsBestModel(df_holdout, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f33513-7586-45b0-ad0a-cb6a32d36700",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_true_best, y_pred_best, alpha=0.4)\n",
    "plt.plot([-1, 1], [-1, 1], color='red', linestyle='--')  # Diag. line\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Predicted Temperature vs. Actual Temperature\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# We need to label these better\n",
    "plt.savefig(f'output/figures/regression_best_model_preds.png')\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905dd786-c907-4250-8e4b-2007636e457e",
   "metadata": {},
   "source": [
    "#### Benchmark 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e12a1d-68a2-44d4-9f0e-12f10debcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_TYPE = 'uniform'\n",
    "\n",
    "df_benchmark = df_holdout.copy()\n",
    "\n",
    "X, y = makeFeaturesTargets(df_benchmark)\n",
    "X = X[ranked_features_top_list]\n",
    "X = X.replace({pd.NA: np.nan})\n",
    "X = X.dropna()\n",
    "\n",
    "# Align dataframe to cleaned X\n",
    "df_benchmark = df_benchmark.loc[X.index]\n",
    "y_true = df_benchmark[TARGET]\n",
    "\n",
    "if BENCHMARK_TYPE == 'uniform':\n",
    "    # Random uniform within observed range\n",
    "    low, high = y_true.min(), y_true.max()\n",
    "    y_dummy = np.random.uniform(low, high, size=len(y_true))\n",
    "elif BENCHMARK_TYPE == 'permutation':\n",
    "    # Random shuffling of true values (permutation)\n",
    "    y_dummy = np.random.permutation(y_true)\n",
    "elif BENCHMARK_TYPE == 'median':\n",
    "    # Median\n",
    "    y_dummy = np.full(len(y_true), np.median(y_true))\n",
    "else:\n",
    "    # Dummy target mean\n",
    "    mean_value = y_true.mean()\n",
    "    y_dummy = [mean_value] * len(y_true)\n",
    "\n",
    "mse = mean_squared_error(y_true, y_dummy)\n",
    "r2 = r2_score(y_true, y_dummy)\n",
    "\n",
    "print(f\"üìâ Dummy {BENCHMARK_TYPE.capitalize()} Squared Error: {mse:.4f}\")\n",
    "print(f\"üìà Dummy {BENCHMARK_TYPE.capitalize()} R¬≤ Score: {r2:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_true, y_dummy, alpha=0.4)\n",
    "plt.plot([-1, 1], [-1, 1], color='red', linestyle='--')  # Diag. line\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Benchmark\")\n",
    "plt.title(\"Benchmark vs. Actual\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# We need to name this better\n",
    "plt.savefig(f'output/figures/regression_benchmark_1.png')\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f74c7a-94d8-4a94-9626-f68b2b776ce1",
   "metadata": {},
   "source": [
    "#### Benchmark 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62830cf1-d0a0-4e8b-b41e-ca92e3292168",
   "metadata": {},
   "outputs": [],
   "source": [
    "RULE_OF_THUMB_FEATURES = ['partisan_temp_prev']\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "df_benchmark = df.copy()\n",
    "\n",
    "# Rule-of-thumb feature\n",
    "rule_features = RULE_OF_THUMB_FEATURES\n",
    "\n",
    "X_rule = df_benchmark[rule_features].copy()\n",
    "y_true = df_benchmark[TARGET].copy()\n",
    "\n",
    "# Drop rows with missing data\n",
    "mask = X_rule.notna().all(axis=1) & y_true.notna()\n",
    "X_rule = X_rule.loc[mask]\n",
    "y_true = y_true.loc[mask]\n",
    "\n",
    "# Fit linear model\n",
    "rule_model = LinearRegression()\n",
    "rule_model.fit(X_rule, y_true)\n",
    "y_pred = rule_model.predict(X_rule)\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(\"Linear Rule-of-Thumb Benchmark\")\n",
    "print(f\"Feature(s) used: {rule_features}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.4)\n",
    "plt.plot([-1, 1], [-1, 1], color='red', linestyle='--')  # Diagonal reference\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted (Rule-of-Thumb)\")\n",
    "plt.title(\"Rule-of-Thumb Prediction vs. Actual\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "# We need to name this better\n",
    "plt.savefig(f'output/figures/regression_benchmark_2.png')\n",
    "plt.close()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735f88c-ae37-407b-ade0-0f54d8cd27c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
