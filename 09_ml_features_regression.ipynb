{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3935347c-8421-4a14-8843-ae6f83d6604e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# President: 2016 (Trump), 2020 (Biden), 2024 (Trump)\n",
    "# U.S. Senate: 2014 (Peters), 2018 (Stabenow), 2020 (Peters), 2024 (Slotkin)\n",
    "# U.S. House: every cycle\n",
    "# State Senate: 2014, 2018, 2022\n",
    "# State House: every cycle\n",
    "\n",
    "ELECTIONS = {}\n",
    "\n",
    "# SKIP FIRST TWO ELECTIONS FOR EVERY OFFICE:\n",
    "ELECTIONS['U.S. House'] =   ['2018', '2020', '2022', '2024']\n",
    "ELECTIONS['State House'] =  ['2018', '2020', '2022', '2024']\n",
    "ELECTIONS['U.S. Senate'] =  ['2020', '2024']\n",
    "ELECTIONS['State Senate'] = ['2022']\n",
    "ELECTIONS['President'] =    ['2024']\n",
    "\n",
    "# NUMERIC ONLY\n",
    "TARGETS = ['partisan_temp']\n",
    "\n",
    "TOP_N_FEATURES = 100\n",
    "TOP_N_FEATURES_TO_DISPLAY = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3398b-028f-4299-92e8-d4ff4292ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18dd8cd-43ef-4c3c-bd8b-b56a35c83763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24eb905-116b-482d-bfbe-795dec4cc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Socioeconomic data as features in addition\n",
    "# to the original and the engineered features.\n",
    "census_datasets = [\n",
    "    'b02001_race', 'b04007_ancestry', 'b05012_nativity_us', 'b08303_travel_time_work', 'b25003_housing_rentership', \n",
    "    'dp02_selected_social_characteristics', 'dp03_selected_economic_characteristics', 'dp04_housing_characteristics', 'dp05_age_race', \n",
    "    's0101_age_sex', 's1101_households_families', 's1201_marital_status', 's1501_educational_attainment', 's1701_income_poverty', \n",
    "    's1903_median_income', 's2101_veteran_status', 's2201_food_stamps', 's2301_employment_status', 's2401_occupation_sex', \n",
    "    's2403_industry_sex', 's2501_occupancy_characteristics', 's2701_health_insurance', 's2503_financial_characteristics',\n",
    "]\n",
    "\n",
    "# These key-like columns just add noise.\n",
    "drop_features_required = [\n",
    "    'standardized_id', 'standardized_id_num',\n",
    "    'aland_tract', 'awater_tract', 'geoid_tract', 'geoidfq_tract', \n",
    "    'geometry', 'geometry_tract', 'name_tract', 'tractce_tract',\n",
    "    'nearest_bound_census_tract', 'nearest_bound_school_district', 'nearest_bound_zipcode',\n",
    "]\n",
    "\n",
    "# Optionally drop one or more of these during \n",
    "# train/test/prediction.\n",
    "\n",
    "# If one of these are a target, they must be enabled in this\n",
    "# list so that it can be dropped later.\n",
    "drop_features_optional = [\n",
    "    'office_code', \n",
    "    'dem_share_prev', \n",
    "    'rep_share_prev', 'oth_share_prev', \n",
    "    'dem_share_change_prev', 'rep_share_change_prev', 'oth_share_change_prev', \n",
    "    'dem_votes_change_prev', 'rep_votes_change_prev', 'oth_votes_change_prev', \n",
    "    'registered_voters_change_prev', 'turnout_pct_change_prev', \n",
    "    'partisan_temp_prev', \n",
    "    'partisan_temp_change_prev', \n",
    "    'partisanship_lean_prev', 'partisanship_lean_change_prev', 'partisanship_lean_change_amount_prev',\n",
    "]\n",
    "\n",
    "# Seen features that may or may not be used as\n",
    "# targets as well.\n",
    "drop_features_seen = [\n",
    "    'dem_votes', 'oth_votes', 'rep_votes', 'total_votes', \n",
    "    'dem_share', 'rep_share', 'oth_share',  'turnout_pct',\n",
    "    'dem_share_change_curr','rep_share_change_curr', 'oth_share_change_curr', \n",
    "    'dem_votes_change_curr','rep_votes_change_curr', 'oth_votes_change_curr', \n",
    "    'partisan_temp', 'partisanship_lean_curr', 'registered_voters',\n",
    "    'registered_voters_change_curr','turnout_pct_change_curr',\n",
    "    'partisan_temp_category', 'partisan_temp_change_curr',\n",
    "    'pedersen_index_percent', 'pedersen_index',\n",
    "    'partisanship_lean_change_amount_curr',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c405e-2ddd-4a0e-b553-2d1a95e1b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # DO NOT EDIT BELOW THIS LINE\n",
    "for target in TARGETS:\n",
    "    if target in drop_features_seen:\n",
    "        drop_features_seen.remove(target) # Keep target in features for later extraction\n",
    "    if target in drop_features_optional:\n",
    "        drop_features_optional.remove(target) # Keep target in features for later extraction\n",
    "\n",
    "drop_features = drop_features_required + drop_features_optional + drop_features_seen\n",
    "\n",
    "# Store dropped features for each target\n",
    "DROP_FEATURES_DICT = {}\n",
    "\n",
    "for target in TARGETS:\n",
    "    drop_features_copy = drop_features.copy()\n",
    "    \n",
    "    if target in drop_features_copy:\n",
    "        drop_features_copy.remove(target)\n",
    "        \n",
    "    DROP_FEATURES_DICT[target] = drop_features_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d37a04a-8a5a-4aac-9255-1616addf0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeUncommonColumns(nested_dict):\n",
    "    print(\"Removing uncommon columns...\")\n",
    "    \n",
    "    # Flatten and find common columns\n",
    "    all_dfs = [df for year in nested_dict for df in nested_dict[year].values()]\n",
    "    common_cols = set(all_dfs[0].columns)\n",
    "    for df in all_dfs[1:]:\n",
    "        common_cols &= set(df.columns)\n",
    "    \n",
    "    # Safely trim all dataframes\n",
    "    for year in nested_dict:\n",
    "        for office in nested_dict[year]:\n",
    "            df = nested_dict[year][office]\n",
    "            existing_cols = [col for col in common_cols if col in df.columns]\n",
    "            nested_dict[year][office] = df[existing_cols]\n",
    "\n",
    "    print('Done.')\n",
    "    \n",
    "    return nested_dict\n",
    "\n",
    "\n",
    "def makeDatasets(years, offices):\n",
    "    print('Making datasets...')\n",
    "    \n",
    "    df_datasets = {}\n",
    "    \n",
    "    for year in years:\n",
    "        print(f'Processing year {year}...')\n",
    "        df_datasets[year] = {}\n",
    "        \n",
    "        for office in offices:\n",
    "            office = office.replace(' ', '_').replace('.', '')\n",
    "            print(f'Processing office {office}...')\n",
    "\n",
    "            df = pd.read_csv('data/generated_data/07_ml_features_' + year + '_' + office + '.csv', low_memory=False)\n",
    "            df_datasets[year][office] = df\n",
    "    \n",
    "    # df_datasets = removeUncommonColumns(df_datasets)\n",
    "    print('Done.')\n",
    "    \n",
    "    return df_datasets\n",
    "\n",
    "\n",
    "def makeFeaturesTargets(df, target):\n",
    "    print(f'Making features and target...')\n",
    "    \n",
    "    y = df[[target]]\n",
    "    \n",
    "    X = df.drop(columns=['standardized_id_num', 'partisan_temp', 'partisan_temp_change_curr']) #KETCHUM aren't we already droppin gthese\n",
    "    X = X.replace(['-', '(X)', 'N/A', 'null', ''], pd.NA)\n",
    "    \n",
    "    X, y = X.align(y.dropna(), join='inner', axis=0)\n",
    "    \n",
    "    print('Done.')\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fitModel(X, y, k=5):\n",
    "    print(f'Fitting model...')\n",
    "\n",
    "    categorical_cols = [\n",
    "        'office_code',\n",
    "        'partisanship_lean_curr',\n",
    "        'partisanship_lean_prev',\n",
    "        'partisanship_lean_change_prev',\n",
    "    ]\n",
    "\n",
    "    # Format the columns\n",
    "    categorical_cols = [col for col in categorical_cols if col in X.columns]\n",
    "    numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col not in categorical_cols]\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore'))\n",
    "    ])\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('cat', categorical_transformer, categorical_cols),\n",
    "        ('num', numeric_transformer, numeric_cols)\n",
    "    ])\n",
    "    model = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    r2_scores = cross_val_score(model, X, y, cv=kf, scoring='r2', n_jobs=-1)\n",
    "    mse_scores = -cross_val_score(model, X, y, cv=kf, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "    print(f'Average R² across {k} folds: {r2_scores.mean():.4f} ± {r2_scores.std():.4f}')\n",
    "    print(f'Average MSE across {k} folds: {mse_scores.mean():.4f} ± {mse_scores.std():.4f}')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print('Final model fitted on training split.')\n",
    "    return model, X_train, X_test, y_train, y_test, numeric_cols\n",
    "\n",
    "\n",
    "def plotAccuracy(y_test, y_pred):\n",
    "    print(f'Plotting accuracy...')\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.4)\n",
    "    plt.xlim(-1.25, 1.25)\n",
    "    plt.ylim(-1.25, 1.25)\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(\"Prediction Accuracy\")\n",
    "    plt.grid(True)\n",
    "    print('Done.')\n",
    "    return plt\n",
    "\n",
    "\n",
    "def featureCoeff(model):\n",
    "    print(f'Computing feature coefficients from pipeline...')\n",
    "\n",
    "    regressor = model.named_steps['regressor']\n",
    "    preprocessor = model.named_steps['preprocessor']\n",
    "    coef = regressor.coef_.flatten()\n",
    "\n",
    "    # Inline feature name extraction\n",
    "    output_features = []\n",
    "    for name, transformer, columns in preprocessor.transformers_:\n",
    "        if transformer == 'drop' or transformer is None:\n",
    "            continue\n",
    "        if hasattr(transformer, 'get_feature_names_out'):\n",
    "            try:\n",
    "                names = transformer.get_feature_names_out(columns)\n",
    "            except:\n",
    "                names = columns\n",
    "        else:\n",
    "            names = columns\n",
    "        output_features.extend(names)\n",
    "\n",
    "    feature_names = output_features\n",
    "\n",
    "    if len(coef) != len(feature_names):\n",
    "        raise ValueError(f\"Mismatch: {len(coef)} coefficients vs {len(feature_names)} feature names\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coef,\n",
    "        'abs_coefficient': np.abs(coef)\n",
    "    })\n",
    "\n",
    "    top_features = df.sort_values(by='abs_coefficient', ascending=False).head(TOP_N_FEATURES_TO_DISPLAY)\n",
    "    print('Done.')\n",
    "    return top_features\n",
    "\n",
    "\n",
    "def plotFeatureCoeff(features):\n",
    "    print(f'Plotting feature coefficients...')\n",
    "    plt.figure(figsize=(12, 18))\n",
    "    bars = plt.barh(features['feature'], features['coefficient'])\n",
    "    plt.xlabel('Coefficient Value')\n",
    "    plt.xticks(fontsize=7)\n",
    "    plt.yticks(fontsize=7)\n",
    "    plt.title(f'Most Influential Features (Linear Regression)')\n",
    "    plt.axvline(x=0, color='gray', linestyle='--')\n",
    "    plt.grid(True, axis='x', linestyle=':', alpha=0.7)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    print('Done.')\n",
    "    return plt\n",
    "\n",
    "\n",
    "def get_feature_names(model):\n",
    "    print(\"Getting feature names...\")\n",
    "\n",
    "    preprocessor = model.named_steps['preprocessor']\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "    # Store both prefixed and cleaned names\n",
    "    cleaned_names = [name.split('__', 1)[-1] for name in feature_names]\n",
    "\n",
    "    print(f\"Done. Retrieved {len(feature_names)} feature names.\")\n",
    "    return feature_names, cleaned_names\n",
    "\n",
    "\n",
    "def mergeTopFeatures(top_features_lists):\n",
    "    print(f'Creating common top features using clean names...')\n",
    "    from itertools import chain\n",
    "\n",
    "    # Make clean names\n",
    "    normalized_lists = []\n",
    "    for item in top_features_lists:\n",
    "        if isinstance(item, list):\n",
    "            normalized_lists.append(item)\n",
    "        elif hasattr(item, 'columns') and 'feature' in item.columns:\n",
    "            normalized_lists.append(item['feature'].tolist())\n",
    "        else:\n",
    "            raise ValueError(\"Each item must be a list or a DataFrame with a 'feature' column\")\n",
    "\n",
    "    # Find intersection\n",
    "    common_features = set(normalized_lists[0])\n",
    "    for feature_list in normalized_lists[1:]:\n",
    "        common_features.intersection_update(feature_list)\n",
    "\n",
    "    # Preserve order\n",
    "    seen = set()\n",
    "    merged_common_ordered = []\n",
    "\n",
    "    for item in chain.from_iterable(normalized_lists):\n",
    "        if item in common_features and item not in seen:\n",
    "            seen.add(item)\n",
    "            merged_common_ordered.append(item)\n",
    "\n",
    "    print('Done.')\n",
    "    return merged_common_ordered\n",
    "\n",
    "\n",
    "def one_hot_encode_selected(df, columns_to_encode):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if not columns_to_encode:\n",
    "        return df\n",
    "        \n",
    "    encoded = pd.get_dummies(df[columns_to_encode], prefix=columns_to_encode)\n",
    "    df = df.drop(columns=columns_to_encode)\n",
    "    \n",
    "    return pd.concat([df, encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430706f-0828-4013-91a3-195f2662f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRankedFeatureList(target):\n",
    "    df_ranked_features = pd.read_csv(f'data/generated_data/df_importances_{target}.csv')\n",
    "    \n",
    "    df_ranked_features = df_ranked_features.sort_values(by='Average', ascending=False)\n",
    "    drop_features = DROP_FEATURES_DICT[target]\n",
    "\n",
    "    df_ranked_features = df_ranked_features[~df_ranked_features['Feature name'].isin(drop_features)]\n",
    "    df_ranked_features_top = df_ranked_features.head(TOP_N_FEATURES)\n",
    "\n",
    "    ranked_features_top_list = df_ranked_features_top['Feature name'].tolist()\n",
    "    \n",
    "    return ranked_features_top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490575b3-eb22-42a4-b9a4-91a0a25e290d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predDatasetsIndiv(df_datasets, years, offices):\n",
    "    for year in years:\n",
    "        print(f'Processing year {year}...')\n",
    "        \n",
    "        for office in offices:\n",
    "            office = office.replace(' ', '_').replace('.', '')\n",
    "            print(f'Processing office {office}...')\n",
    "    \n",
    "            df = df_datasets[year][office].copy()\n",
    "\n",
    "            for target in TARGETS:\n",
    "                print(f'Processing target {target}...')\n",
    "\n",
    "                X, y = makeFeaturesTargets(df, target)\n",
    "                ranked_features_top_list = getRankedFeatureList(target).copy()\n",
    "\n",
    "                if target in ranked_features_top_list:\n",
    "                    ranked_features_top_list.remove(target)\n",
    "                \n",
    "                features_to_exclude = set(TARGETS)\n",
    "                ranked_features_top_list = [f for f in ranked_features_top_list if f not in features_to_exclude]\n",
    "                \n",
    "                X = X[ranked_features_top_list]\n",
    "        \n",
    "                print(f\"Training over {len(X.columns)} features...\")\n",
    "                \n",
    "                model, X_train, X_test, y_train, y_test, numeric_cols = fitModel(X, y)\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "                \n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                print(\"Mean Squared Error:\", mse)\n",
    "        \n",
    "                r2 = model.score(X_test, y_test)\n",
    "                print(\"R2 Score:\", r2)\n",
    "        \n",
    "                plt = plotAccuracy(y_test, y_pred)\n",
    "                plt.savefig(f'output/figures/regression_accuracy_{year}_{office}_{target}_individual.png')\n",
    "                # plt.show()\n",
    "                plt.close()\n",
    "                \n",
    "                top_features = featureCoeff(model)\n",
    "                plt = plotFeatureCoeff(top_features)\n",
    "                plt.savefig(f'output/figures/regression_top_features_{year}_{office}_{target}_individual.png')\n",
    "                # plt.show()\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513574a-ab5e-4dcd-849c-675203956a52",
   "metadata": {},
   "source": [
    "#### Predict Individual Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb48fc8-b90a-42cf-94ac-e86a61a277bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Num. of offices to process: {len(ELECTIONS)}')\n",
    "\n",
    "for key, value in ELECTIONS.items():\n",
    "    print(f'Num. of years to process: {len(value)}')\n",
    "    \n",
    "    OFFICES = [key]\n",
    "    YEARS = value\n",
    "\n",
    "    print(f'Process office(s): {key} for year(s): {', '.join(YEARS)}')\n",
    "\n",
    "    df_datasets = makeDatasets(YEARS, OFFICES)\n",
    "    predDatasetsIndiv(df_datasets, YEARS, OFFICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adc1e41-2798-4c30-a4c6-0c902b1edcc9",
   "metadata": {},
   "source": [
    "#### Fit/Train Final Model\n",
    "Do we have data leakage here? Maybe not, if we train on historical data and\n",
    "<br>run a separate test on newer data in a following step. Say, here we\n",
    "train 2018-2022, <br>and then in another cell test 2024 on the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb5caa4-538d-44f2-aaa0-de064e1a6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLDOUT_YEAR = '2024'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bd73f-b048-420e-ae16-dedcb315983f",
   "metadata": {},
   "source": [
    "#### Fit aggregated data\n",
    "This functionality produces the <code>model</code> object to be used later. Make sure no datasets from this\n",
    "<br>cell is NOT included in holdout testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feff2391-6737-4fd6-bbef-7c016a99101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggDatasets(df_datasets, years, offices):\n",
    "    dfs = []\n",
    "    \n",
    "    for year in years:\n",
    "        print(f'Processing year {year}...')\n",
    "        for office in offices:\n",
    "            office = office.replace(' ', '_').replace('.', '')\n",
    "            \n",
    "            print(f'Processing office {office}...')\n",
    "            dfs.append(df_datasets[year][office].copy())\n",
    "    if len(dfs) > 1:\n",
    "        df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    elif len(dfs) == 0:\n",
    "        raise ValueError(\"No dataframes found to aggregate.\")\n",
    "    elif len(dfs) == 1:\n",
    "        return dfs[0]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a898b8-e2d1-4568-b1c5-9b9c2aa7c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAggDataset(holdout_year):\n",
    "    df_agg_dataset_list = []\n",
    "    \n",
    "    for office, years in ELECTIONS.items():\n",
    "        for year in years:\n",
    "            if year == holdout_year:\n",
    "                continue\n",
    "            office = office.replace(' ', '_').replace('.', '')\n",
    "            df = pd.read_csv('data/generated_data/07_ml_features_' + year + '_' + office + '.csv', low_memory=False)\n",
    "            df_agg_dataset_list.append(df)\n",
    "    \n",
    "    df_agg = pd.concat(df_agg_dataset_list, axis=0, ignore_index=True)\n",
    "\n",
    "    return df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7dede2-2ded-4579-bf0c-6f316f09d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = getAggDataset(HOLDOUT_YEAR)\n",
    "\n",
    "model_dict = {}\n",
    "\n",
    "for target in TARGETS:\n",
    "    \n",
    "    X, y = makeFeaturesTargets(df_agg, target)\n",
    "\n",
    "    ranked_features_top_list = getRankedFeatureList(target)\n",
    "    \n",
    "    if target in ranked_features_top_list:\n",
    "        ranked_features_top_list.remove(target)\n",
    "    \n",
    "    features_to_exclude = set(TARGETS)\n",
    "    ranked_features_top_list = [f for f in ranked_features_top_list if f not in features_to_exclude]\n",
    "    \n",
    "    X = X[ranked_features_top_list]\n",
    "    X = X.replace({pd.NA: np.nan})\n",
    "    \n",
    "    model, X_train, X_test, y_train, y_test, numeric_cols = fitModel(X, y)\n",
    "\n",
    "    model_dict[target] = model\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    expected_columns = X.columns.tolist()\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    \n",
    "    r2 = model.score(X_test, y_test)\n",
    "    print(\"R2 Score:\", r2)\n",
    "    \n",
    "    plt = plotAccuracy(y_test, y_pred)\n",
    "    plt.savefig(f'output/figures/regression_accuracy_{target}_aggregate.png')\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "    \n",
    "    top_features = featureCoeff(model)\n",
    "    plt = plotFeatureCoeff(top_features)\n",
    "    plt.savefig(f'output/figures/regression_top_features_{target}_aggregate.png')\n",
    "    plt.close()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9beb1b-c5a0-4485-bf8c-bc0a243c06f0",
   "metadata": {},
   "source": [
    "#### Holdout Prediction\n",
    "This functionality requires the <code>model_dict</code> object from a previous cell. Make sure this holdout\n",
    "<br>dataset was not included in the model's training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856ecb9-9115-48ca-bc1c-45b3ce98ba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHoldoutDataset(holdout_year=HOLDOUT_YEAR):\n",
    "    df_holdout_dataset_list = []\n",
    "    \n",
    "    for office, years in ELECTIONS.items():\n",
    "        for year in years:\n",
    "            if year == holdout_year:\n",
    "                office = office.replace(' ', '_').replace('.', '')\n",
    "                df = pd.read_csv('data/generated_data/07_ml_features_' + year + '_' + office + '.csv', low_memory=False)\n",
    "                df_holdout_dataset_list.append(df)\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    df_holdout = pd.concat(df_holdout_dataset_list, axis=0, ignore_index=True)\n",
    "\n",
    "    return df_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da35164d-cf66-4d6e-bfbd-4b32e5e52b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holdout = getHoldoutDataset(HOLDOUT_YEAR)\n",
    "\n",
    "for target in TARGETS:\n",
    "    \n",
    "    df_orig = df_holdout.copy()\n",
    "    \n",
    "    X, y = makeFeaturesTargets(df_holdout, target)\n",
    "    \n",
    "    ranked_features_top_list = getRankedFeatureList(target)\n",
    "    \n",
    "    if target in ranked_features_top_list:\n",
    "        ranked_features_top_list.remove(target)\n",
    "    \n",
    "    features_to_exclude = set(TARGETS)\n",
    "    ranked_features_top_list = [f for f in ranked_features_top_list if f not in features_to_exclude]\n",
    "    \n",
    "    X = X[ranked_features_top_list]\n",
    "    \n",
    "    X = X.replace({pd.NA: np.nan})\n",
    "        \n",
    "    model = model_dict[target]\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    expected_columns = X.columns.tolist()\n",
    "    \n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    \n",
    "    r2 = model.score(X, y)\n",
    "    print(\"R2 Score:\", r2)\n",
    "    \n",
    "    print(f\"Holdout MSE ({HOLDOUT_YEAR}):\", mse)\n",
    "    print(f\"Holdout R² ({HOLDOUT_YEAR}):\", r2)\n",
    "    \n",
    "    with open(f'output/reports/prediction_eval_{target}_holdout_regression.txt', 'w') as f:\n",
    "        output = f\"R2 Score: {r2}\"\n",
    "        output += f\"\\nMSE: {mse}\"\n",
    "        f.write(output)\n",
    "    \n",
    "    plt = plotAccuracy(y, y_pred)\n",
    "    plt.savefig(f'output/figures/regression_accuracy_{target}_holdout.png')\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "    \n",
    "    top_features = featureCoeff(model)\n",
    "    plt = plotFeatureCoeff(top_features)\n",
    "    plt.savefig(f'output/figures/regressio_top_features_{target}_holdout.png')\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "    \n",
    "    # Save holdout predictions\n",
    "    df_orig['predicted_label'] = pd.Series(y_pred.ravel(), index=X.index)\n",
    "    df_export = df_orig[[\"standardized_id_num\", target, \"predicted_label\"]].copy()\n",
    "    df_export[\"standardized_id_num\"] = df_export[\"standardized_id_num\"].astype(str).str.replace('.0', '').str.zfill(13)\n",
    "    df_export.rename(columns={target: 'true_label'}, inplace=True)\n",
    "    \n",
    "    filename = f\"data/generated_data/predicted_{target}_holdout.csv\"\n",
    "    df_export.to_csv(filename, index=False)\n",
    "    print(f\"Holdout predictions saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a128ef0-988e-4632-ae09-1594510ec32a",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197ad5f8-313f-4da8-a88c-6570cc20fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "def gridSearch(df, target, ranked_features_top_list):\n",
    "    df = df.copy()\n",
    "    \n",
    "    X, y = makeFeaturesTargets(df, target)\n",
    "\n",
    "    if target in ranked_features_top_list:\n",
    "        ranked_features_top_list.remove(target)\n",
    "    \n",
    "    features_to_exclude = set(TARGETS)\n",
    "    ranked_features_top_list = [f for f in ranked_features_top_list if f not in features_to_exclude]\n",
    "    \n",
    "    X = X[ranked_features_top_list]\n",
    "    X = X.replace({pd.NA: np.nan})\n",
    "    \n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "    \n",
    "    models = {\n",
    "        'LinearRegression': (\n",
    "            LinearRegression(), \n",
    "            {}\n",
    "        ),\n",
    "        'Ridge': (\n",
    "            Ridge(),\n",
    "            {'regressor__alpha': [0.01, 0.1, 1.0, 10]}\n",
    "        ),\n",
    "        'Lasso': (\n",
    "            Lasso(max_iter=10000),\n",
    "            {'regressor__alpha': [0.01, 0.1, 1.0, 10]}\n",
    "        ),\n",
    "        'ElasticNet': (\n",
    "            ElasticNet(max_iter=10000), \n",
    "            {\n",
    "            'regressor__alpha': [0.01, 0.1, 1.0],\n",
    "            'regressor__l1_ratio': [0.2, 0.5, 0.8]\n",
    "            }\n",
    "        ),\n",
    "        'DecisionTree': (\n",
    "            DecisionTreeRegressor(), \n",
    "            {\n",
    "            'regressor__max_depth': [5, 10, None],\n",
    "            'regressor__min_samples_split': [2, 10]\n",
    "            }\n",
    "        ),\n",
    "        'RandomForest': (\n",
    "            RandomForestRegressor(n_jobs=-1), \n",
    "            {\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__max_depth': [5, 10, None]\n",
    "            }\n",
    "        ),\n",
    "        'GradientBoosting': (\n",
    "            GradientBoostingRegressor(), \n",
    "            {\n",
    "            'regressor__n_estimators': [50, 100],\n",
    "            'regressor__learning_rate': [0.05, 0.1],\n",
    "            'regressor__max_depth': [3, 5]\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    y = y.values.ravel() \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    best_models = {}\n",
    "    results = []\n",
    "    \n",
    "    for name, (model, params) in models.items():\n",
    "        print(f\"Tuning: {name}\")\n",
    "        \n",
    "        pipe = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', model)\n",
    "        ])\n",
    "        \n",
    "        grid = GridSearchCV(pipe, param_grid=params, cv=5, scoring='r2', n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "    \n",
    "        best_model = grid.best_estimator_\n",
    "        y_pred = best_model.predict(X_test)\n",
    "    \n",
    "        mse = mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Best Params': grid.best_params_,\n",
    "            'MSE': mse,\n",
    "            'R2': r2\n",
    "        })\n",
    "    \n",
    "        best_models[name] = grid \n",
    "    \n",
    "    results_df = pd.DataFrame(results).sort_values(by='R2', ascending=False)\n",
    "\n",
    "    print('Done.')\n",
    "\n",
    "    return best_models, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd77a93-9de0-4a8a-b4ce-b9f2117eb288",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = getAggDataset(HOLDOUT_YEAR)\n",
    "\n",
    "best_models_dict = {}\n",
    "results_df_dict = {}\n",
    "\n",
    "for target in TARGETS:    \n",
    "    ranked_features_top_list = getRankedFeatureList(target)\n",
    "    best_models, results_df = gridSearch(df_agg, target, ranked_features_top_list)\n",
    "\n",
    "    best_models_dict[target] = best_models\n",
    "    results_df_dict[target] = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11416d-42e5-46b4-b137-8b06799277e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitBestModel(df, target, params, ranked_features_top_list):\n",
    "    df = df.copy()\n",
    "    \n",
    "    X, y = makeFeaturesTargets(df, target)\n",
    "\n",
    "    if target in ranked_features_top_list:\n",
    "        ranked_features_top_list.remove(target)\n",
    "    \n",
    "    features_to_exclude = set(TARGETS)\n",
    "    ranked_features_top_list = [f for f in ranked_features_top_list if f not in features_to_exclude]\n",
    "    \n",
    "    X = X[ranked_features_top_list]\n",
    "    X = X.replace({pd.NA: np.nan})\n",
    "    \n",
    "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    numeric_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "    \n",
    "    gbr_final = GradientBoostingRegressor(\n",
    "        random_state=42,\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('regressor', gbr_final)\n",
    "    ])\n",
    "    \n",
    "    model.fit(X, y.values.ravel())\n",
    "\n",
    "    print('Done.')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9cbbe-7b78-4c50-b516-9babe9d2fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanParams(best_models, model_name):\n",
    "    params = best_models[model_name].best_params_\n",
    "    clean_params = {k.replace('regressor__', ''): v for k, v in params.items()}\n",
    "    \n",
    "    print(f'Cleaned params for {model_name}: {clean_params}')\n",
    "    \n",
    "    return clean_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6489a0c-14a7-402f-93ce-33639e22a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = getAggDataset(HOLDOUT_YEAR)\n",
    "\n",
    "fitted_models_dict = {}\n",
    "\n",
    "for target in TARGETS:\n",
    "    clean_params = cleanParams(best_models, 'GradientBoosting')\n",
    "    ranked_features_top_list = getRankedFeatureList(target)\n",
    "    model = fitBestModel(df_agg, target, clean_params, ranked_features_top_list)\n",
    "    fitted_models_dict[target] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011cd4e5-b222-4801-b9eb-22c37334a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePredsBestModel(df, target, model, ranked_features_top_list):\n",
    "    df = df.copy()\n",
    "    \n",
    "    X, y = makeFeaturesTargets(df, target)\n",
    "\n",
    "    ranked_features_top_list = [f.strip() for f in ranked_features_top_list if f.strip() not in TARGETS]\n",
    "    ranked_features_top_list = [f for f in ranked_features_top_list if f in X.columns]\n",
    "    if not ranked_features_top_list:\n",
    "        raise ValueError(\"No valid features left for prediction — check feature list or holdout data.\")\n",
    "\n",
    "    X = X[ranked_features_top_list]\n",
    "    X = X.replace({pd.NA: np.nan})\n",
    "    X = X.dropna()\n",
    "    \n",
    "    # Align datagrame to cleaned X\n",
    "    df = df.loc[X.index]\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    y_true = df[target]\n",
    "    \n",
    "    df[f\"predicted_{target}\"] = y_pred\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "\n",
    "    return df, y_pred, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b71a2d-56a4-4fad-810a-242f1593b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_holdout = getHoldoutDataset(HOLDOUT_YEAR)\n",
    "\n",
    "for target in TARGETS:\n",
    "    print(f'Processing target {target}...')\n",
    "\n",
    "    ranked_features_top_list = getRankedFeatureList(target)\n",
    "\n",
    "    model = fitted_models_dict[target]\n",
    "\n",
    "    df_pred_best, y_pred_best, y_true_best = makePredsBestModel(df_holdout, target, model, ranked_features_top_list)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true_best, y_pred_best, alpha=0.4)\n",
    "    plt.plot([-1, 1], [-1, 1], color='red', linestyle='--')  # Diag. line\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Predicted Temperature vs. Actual Temperature\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # We need to label these better\n",
    "    plt.savefig(f'output/figures/regression_best_model_{target}_{HOLDOUT_YEAR}_holdout_preds.png')\n",
    "    plt.close()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905dd786-c907-4250-8e4b-2007636e457e",
   "metadata": {},
   "source": [
    "#### Benchmark 1 Uniform (U.S. House 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bfa82-b124-4472-a49c-667486df3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [HOLDOUT_YEAR]\n",
    "offices = ['U.S. House']\n",
    "dfs = makeDatasets(years, offices)\n",
    "\n",
    "df_benchmark = dfs[HOLDOUT_YEAR][offices[0].replace(' ', '_').replace('.', '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e12a1d-68a2-44d4-9f0e-12f10debcb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_TYPE = 'uniform'\n",
    "\n",
    "df_benchmark1 = df_benchmark.copy()\n",
    "\n",
    "for target in TARGETS:\n",
    "    \n",
    "    X, y = makeFeaturesTargets(df_benchmark1, target)\n",
    "\n",
    "    if target in ranked_features_top_list:\n",
    "        ranked_features_top_list.remove(target)\n",
    "    \n",
    "    features_to_exclude = set(TARGETS)\n",
    "    ranked_features_top_list = [f for f in ranked_features_top_list if f not in features_to_exclude]\n",
    "    \n",
    "    X = X[ranked_features_top_list]\n",
    "    X = X.replace({pd.NA: np.nan})\n",
    "    X = X.dropna()\n",
    "    \n",
    "    df_benchmark1 = df_benchmark1.loc[X.index]\n",
    "    y_true = df_benchmark1[target]\n",
    "    \n",
    "    if BENCHMARK_TYPE == 'uniform':\n",
    "        low, high = y_true.min(), y_true.max()\n",
    "        y_dummy = np.random.uniform(low, high, size=len(y_true))\n",
    "    elif BENCHMARK_TYPE == 'permutation':\n",
    "        y_dummy = np.random.permutation(y_true)\n",
    "    elif BENCHMARK_TYPE == 'median':\n",
    "        y_dummy = np.full(len(y_true), np.median(y_true))\n",
    "    else:\n",
    "        mean_value = y_true.mean()\n",
    "        y_dummy = [mean_value] * len(y_true)\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_dummy)\n",
    "    r2 = r2_score(y_true, y_dummy)\n",
    "    \n",
    "    print(f\"Dummy {BENCHMARK_TYPE.capitalize()} Squared Error: {mse:.4f}\")\n",
    "    print(f\"Dummy {BENCHMARK_TYPE.capitalize()} R² Score: {r2:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_dummy, alpha=0.4)\n",
    "    plt.plot([-1, 1], [-1, 1], color='red', linestyle='--')  # Diag. line\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Benchmark\")\n",
    "    plt.title(\"Benchmark vs. Actual\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # We need to name this better\n",
    "    plt.savefig(f'output/figures/regression_benchmark_{BENCHMARK_TYPE}_{target}_1.png')\n",
    "    plt.close()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f74c7a-94d8-4a94-9626-f68b2b776ce1",
   "metadata": {},
   "source": [
    "#### Benchmark 2 Rule-of-Thumb (U.S. House 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62830cf1-d0a0-4e8b-b41e-ca92e3292168",
   "metadata": {},
   "outputs": [],
   "source": [
    "RULE_OF_THUMB_FEATURES = [\n",
    "    'partisan_temp', \n",
    "    'partisan_temp_prev'\n",
    "]\n",
    "\n",
    "COMBINE_RULE_OF_THUMB_FEATURES = False\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "df_benchmark2 = df_benchmark.copy()\n",
    "\n",
    "if COMBINE_RULE_OF_THUMB_FEATURES == False:\n",
    "    rule_features_list = RULE_OF_THUMB_FEATURES\n",
    "else:\n",
    "    rule_features_list = [RULE_OF_THUMB_FEATURES]\n",
    "\n",
    "for rule_features in rule_features_list:\n",
    "    if COMBINE_RULE_OF_THUMB_FEATURES == True:\n",
    "        X_rule = df_benchmark2[rule_features].copy()\n",
    "    else:\n",
    "        X_rule = df_benchmark2[[rule_features]].copy()\n",
    "    \n",
    "    y_true = df_benchmark2[target].copy()\n",
    "    \n",
    "    mask = X_rule.notna().all(axis=1) & y_true.notna()\n",
    "    X_rule = X_rule.loc[mask]\n",
    "    y_true = y_true.loc[mask]\n",
    "    \n",
    "    rule_model = LinearRegression()\n",
    "    rule_model.fit(X_rule, y_true)\n",
    "    y_pred = rule_model.predict(X_rule)\n",
    "    \n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(\"Linear Rule-of-Thumb Benchmark\")\n",
    "    print(f\"Feature(s) used: {rule_features}\")\n",
    "    print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.4)\n",
    "    plt.plot([-1, 1], [-1, 1], color='red', linestyle='--')  # Diagonal reference\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted (Rule-of-Thumb)\")\n",
    "    plt.title(\"Rule-of-Thumb Prediction vs. Actual\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # We need to name this better\n",
    "    plt.savefig(f'output/figures/regression_benchmark_linear_{target}_2.png')\n",
    "    plt.close()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735f88c-ae37-407b-ade0-0f54d8cd27c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
