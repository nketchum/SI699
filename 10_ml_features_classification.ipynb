{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31078ec3-c0d0-487f-a873-800115c25693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# President: 2016 (Trump), 2020 (Biden), 2024 (Trump)\n",
    "# U.S. Senate: 2014 (Peters), 2018 (Stabenow), 2020 (Peters), 2024 (Slotkin)\n",
    "# U.S. House: every cycle\n",
    "# State Senate: 2014, 2018, 2022\n",
    "# State House: every cycle\n",
    "\n",
    "ELECTIONS = {}\n",
    "\n",
    "# SKIP FIRST TWO ELECTIONS FOR EVERY OFFICE:\n",
    "ELECTIONS['U.S. House'] =   ['2018', '2020', '2022', '2024']\n",
    "ELECTIONS['State House'] =  ['2018', '2020', '2022', '2024']\n",
    "ELECTIONS['U.S. Senate'] =  ['2020', '2024']\n",
    "ELECTIONS['State Senate'] = ['2022']\n",
    "ELECTIONS['President'] =    ['2024']\n",
    "\n",
    "TARGETS = ['partisanship_lean_curr']\n",
    "\n",
    "TOP_N_FEATURES = 20\n",
    "FEATURES_ALREADY_RANKED = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3398b-028f-4299-92e8-d4ff4292ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import r2_score, accuracy_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18dd8cd-43ef-4c3c-bd8b-b56a35c83763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd2fec-2a66-45dc-bfe0-951f4db2364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Socioeconomic data as features in addition\n",
    "# to the original and the engineered features.\n",
    "census_datasets = [\n",
    "    'b02001_race', 'b04007_ancestry', 'b05012_nativity_us', 'b08303_travel_time_work', 'b25003_housing_rentership', \n",
    "    'dp02_selected_social_characteristics', 'dp03_selected_economic_characteristics', 'dp04_housing_characteristics', 'dp05_age_race', \n",
    "    's0101_age_sex', 's1101_households_families', 's1201_marital_status', 's1501_educational_attainment', 's1701_income_poverty', \n",
    "    's1903_median_income', 's2101_veteran_status', 's2201_food_stamps', 's2301_employment_status', 's2401_occupation_sex', \n",
    "    's2403_industry_sex', 's2501_occupancy_characteristics', 's2701_health_insurance', 's2503_financial_characteristics',\n",
    "]\n",
    "\n",
    "# These key-like columns just add noise.\n",
    "drop_features_required = [\n",
    "    'standardized_id', 'standardized_id_num',\n",
    "    'aland_tract', 'awater_tract', 'geoid_tract', 'geoidfq_tract', \n",
    "    'geometry', 'geometry_tract', 'name_tract', 'tractce_tract',\n",
    "    'nearest_bound_census_tract', 'nearest_bound_school_district', 'nearest_bound_zipcode',\n",
    "]\n",
    "\n",
    "# If one of these are a target, they must be enabled in this\n",
    "# list so that it can be dropped later.\n",
    "drop_features_optional = [\n",
    "    'office_code', \n",
    "    'dem_share_prev', \n",
    "    'rep_share_prev', 'oth_share_prev', \n",
    "    'dem_share_change_prev', 'rep_share_change_prev', 'oth_share_change_prev', \n",
    "    'dem_votes_change_prev', 'rep_votes_change_prev', 'oth_votes_change_prev', \n",
    "    'registered_voters_change_prev', 'turnout_pct_change_prev', \n",
    "    'partisan_temp_prev', \n",
    "    'partisan_temp_change_prev', \n",
    "    'partisanship_lean_prev', 'partisanship_lean_change_prev', 'partisanship_lean_change_amount_prev',\n",
    "]\n",
    "\n",
    "# Seen features that may or may not be used as\n",
    "# targets as well.\n",
    "drop_features_seen = [\n",
    "    'dem_votes', 'oth_votes', 'rep_votes', 'total_votes', \n",
    "    'dem_share', 'rep_share', 'oth_share',  'turnout_pct',\n",
    "    'dem_share_change_curr','rep_share_change_curr', 'oth_share_change_curr', \n",
    "    'dem_votes_change_curr','rep_votes_change_curr', 'oth_votes_change_curr', \n",
    "    'partisan_temp', 'partisanship_lean_curr', 'registered_voters',\n",
    "    'registered_voters_change_curr','turnout_pct_change_curr',\n",
    "    'partisan_temp_category', 'partisan_temp_change_curr',\n",
    "    'pedersen_index_percent', 'pedersen_index',\n",
    "    'partisanship_lean_change_amount_curr',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95e4c8-6ace-41bd-a9cd-a42d9c2f11a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT BELOW THIS LINE\n",
    "for target in TARGETS:\n",
    "    if target in drop_features_seen:\n",
    "        drop_features_seen.remove(target) # Keep target in features for later extraction\n",
    "    if target in drop_features_optional:\n",
    "        drop_features_optional.remove(target) # Keep target in features for later extraction\n",
    "\n",
    "drop_features = drop_features_required + drop_features_optional + drop_features_seen\n",
    "\n",
    "# Store dropped features for each target\n",
    "DROP_FEATURES_DICT = {}\n",
    "\n",
    "for target in TARGETS:\n",
    "    drop_features_copy = drop_features.copy()\n",
    "    \n",
    "    if target in drop_features_copy:\n",
    "        drop_features_copy.remove(target)\n",
    "        \n",
    "    DROP_FEATURES_DICT[target] = drop_features_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffc9d13-ffd7-4cb8-8bfb-05d937860687",
   "metadata": {},
   "source": [
    "### XG Boost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc210d1-db3e-443f-8e44-9c707952e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Pull the engineered feature data along with its\n",
    "    target for each year and office.'''\n",
    "def makeDatasets(years, offices):\n",
    "    print('Making datasets...')\n",
    "    \n",
    "    df_datasets = {}\n",
    "    \n",
    "    for year in years:\n",
    "        print(f'Processing year {year}...')\n",
    "        df_datasets[year] = {}\n",
    "        \n",
    "        for office in offices:\n",
    "            office = office.replace(' ', '_').replace('.', '')\n",
    "            print(f'Processing office {office}...')\n",
    "\n",
    "            df = pd.read_csv('data/generated_data/07_ml_features_' + year + '_' + office + '.csv', low_memory=False)\n",
    "            df_datasets[year][office] = df\n",
    "    \n",
    "    df_datasets = removeUncommonColumns(df_datasets)\n",
    "    print('Done.')\n",
    "    \n",
    "    return df_datasets\n",
    "\n",
    "\n",
    "def aggDatasets(df_datasets, years, offices):\n",
    "    dfs = []\n",
    "    \n",
    "    for year in years:\n",
    "        print(f'Processing year {year}...')\n",
    "        for office in offices:\n",
    "            office = office.replace(' ', '_').replace('.', '')\n",
    "            \n",
    "            print(f'Processing office {office}...')\n",
    "            dfs.append(df_datasets[year][office].copy())\n",
    "    if len(dfs) > 1:\n",
    "        df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    elif len(dfs) == 0:\n",
    "        raise ValueError(\"No dataframes found to aggregate.\")\n",
    "    elif len(dfs) == 1:\n",
    "        return dfs[0]\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "''' Remove top features not shared between \n",
    "    different datasets to prevent errors.'''\n",
    "def removeUncommonColumns(nested_dict):\n",
    "    print(\"Removing uncommon columns...\")\n",
    "    \n",
    "    # Flatten and find common columns\n",
    "    all_dfs = [df for year in nested_dict for df in nested_dict[year].values()]\n",
    "    common_cols = set(all_dfs[0].columns)\n",
    "    for df in all_dfs[1:]:\n",
    "        common_cols &= set(df.columns)\n",
    "    \n",
    "    # Safely trim all dataframes\n",
    "    for year in nested_dict:\n",
    "        for office in nested_dict[year]:\n",
    "            df = nested_dict[year][office]\n",
    "            existing_cols = [col for col in common_cols if col in df.columns]\n",
    "            nested_dict[year][office] = df[existing_cols]\n",
    "\n",
    "    print('Done.')\n",
    "    \n",
    "    return nested_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd377e-6389-4dfd-afe2-852da91b38ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INDIVIDUAL ELECTIONS PER OFFICE/YEAR\n",
    "print(f'Num. of offices to process: {len(ELECTIONS)}')\n",
    "\n",
    "for key, value in ELECTIONS.items():\n",
    "    print(f'Num. of years to process: {len(value)}')\n",
    "    \n",
    "    OFFICES = [key]\n",
    "    YEARS = value\n",
    "\n",
    "    print(f'Process office(s): {key} for year(s): {', '.join(YEARS)}')\n",
    "    \n",
    "    for year in YEARS:\n",
    "        print(f'Processing year {year}...')\n",
    "    \n",
    "        for office in OFFICES:\n",
    "            office = office.replace(' ', '_').replace('.', '')\n",
    "            print(f'Processing office {office}...')\n",
    "            \n",
    "            df = pd.read_csv(f'data/generated_data/07_ml_features_{year}_{office}.csv')\n",
    "\n",
    "            for target in TARGETS:\n",
    "                print(f'Processing target {target}')\n",
    "            \n",
    "                if FEATURES_ALREADY_RANKED:\n",
    "                    print(f'Features already ranked, select top...')\n",
    "                    feature_importance_file = f'data/generated_data/df_importances_{target}.csv'\n",
    "                    top_feature_columns = pd.read_csv(feature_importance_file)['Feature name'].head(TOP_N_FEATURES).tolist()\n",
    "                    required_columns = ['standardized_id_num', target]\n",
    "                    selected_columns = [col for col in top_feature_columns + required_columns if col in df.columns]\n",
    "                    df = df[selected_columns].dropna(subset=[target])\n",
    "                \n",
    "                # Clean ID\n",
    "                df['standardized_id_num'] = df['standardized_id_num'].astype(str).str.zfill(13)\n",
    "                \n",
    "                # Drop rows w/o targets\n",
    "                df = df.dropna(subset=[target])\n",
    "                \n",
    "                # Save standardized IDs\n",
    "                X_stids = df['standardized_id_num']\n",
    "                \n",
    "                # Define y and encode\n",
    "                y = df[target]\n",
    "                label_encoder = LabelEncoder()\n",
    "                y_encoded = label_encoder.fit_transform(y)\n",
    "                \n",
    "                # Drop unneeded columns after saving id\n",
    "                cols_to_drop = [col for col in drop_features if col in df.columns]\n",
    "                df = df.drop(columns=cols_to_drop)\n",
    "                X = df.drop(columns=[target])\n",
    "                \n",
    "                categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "                numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "                \n",
    "                for col in categorical_cols:\n",
    "                    X[col] = X[col].astype(str)\n",
    "                \n",
    "                categorical_transformer = Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "                ])\n",
    "                \n",
    "                numeric_transformer = Pipeline([\n",
    "                    ('imputer', SimpleImputer(strategy='mean')),\n",
    "                    ('scaler', StandardScaler())\n",
    "                ])\n",
    "                \n",
    "                preprocessor = ColumnTransformer([\n",
    "                    ('cat', categorical_transformer, categorical_cols),\n",
    "                    ('num', numeric_transformer, numeric_cols)\n",
    "                ])\n",
    "                \n",
    "                model = XGBClassifier(\n",
    "                    objective=\"multi:softmax\",  # multi:softmax or multi:softprob\n",
    "                    num_class=len(y.unique()),\n",
    "                    use_label_encoder=False,\n",
    "                    n_estimators=200,\n",
    "                    max_depth=5,\n",
    "                    learning_rate=0.1,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1,\n",
    "                    eval_metric=\"mlogloss\"\n",
    "                )\n",
    "                \n",
    "                model = Pipeline([\n",
    "                    ('preprocessor', preprocessor),\n",
    "                    ('classifier', model)\n",
    "                ])\n",
    "\n",
    "                class_counts = pd.Series(y).value_counts()\n",
    "\n",
    "                valid_classes = class_counts[class_counts > 1].index\n",
    "\n",
    "                mask = y.isin(valid_classes)\n",
    "                X = X[mask]\n",
    "                y = y[mask]\n",
    "                y_encoded = label_encoder.fit_transform(y)\n",
    "                \n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y, random_state=42)\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                y_pred = model.predict(X_test)\n",
    "        \n",
    "                decoded_y_test = label_encoder.inverse_transform(y_test)\n",
    "                decoded_y_pred = label_encoder.inverse_transform(y_pred)\n",
    "                \n",
    "                print(classification_report(decoded_y_test, decoded_y_pred))\n",
    "\n",
    "                cm = confusion_matrix(decoded_y_test, decoded_y_pred)\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                            xticklabels=label_encoder.classes_, \n",
    "                            yticklabels=label_encoder.classes_)\n",
    "                \n",
    "                plt.xlabel('Predicted')\n",
    "                plt.ylabel('True')\n",
    "                plt.title('Confusion Matrix')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'output/figures/confusion_matrix_{year}_{office}.png')\n",
    "                plt.close()\n",
    "                # plt.show()\n",
    "                \n",
    "                # Save predictions\n",
    "                results_df = pd.DataFrame({\n",
    "                    'standardized_id_num': X_stids.loc[X_test.index],\n",
    "                    'true_label': decoded_y_test,\n",
    "                    'predicted_label': decoded_y_pred\n",
    "                })\n",
    "                results_df['standardized_id_num'] = results_df['standardized_id_num'].astype(str).str.zfill(13)\n",
    "                \n",
    "                filename = f'data/generated_data/prediction_results_{year}_{office}_classification.csv'\n",
    "                results_df.to_csv(filename, index=False)\n",
    "        \n",
    "                #############################\n",
    "                # FEATURE PERFORMANCE\n",
    "                #############################\n",
    "                \n",
    "                feature_performance = []\n",
    "                is_continuous = y.dtype.kind in 'fc' # float or continuous\n",
    "                \n",
    "                for feature in tqdm(X.columns):\n",
    "                    X_feature = X[[feature]].copy()\n",
    "                    \n",
    "                    # Handle missing values\n",
    "                    if X_feature[feature].dtype == 'object':\n",
    "                        X_feature = X_feature.fillna(X_feature.mode().iloc[0])\n",
    "                        X_feature = pd.get_dummies(X_feature, drop_first=True)\n",
    "                    \n",
    "                        # Need at least 1 column after one-hot encoding\n",
    "                        if X_feature.shape[1] == 0:\n",
    "                            continue  # Leave as is\n",
    "                    else:\n",
    "                        X_feature = X_feature.fillna(X_feature.mean(numeric_only=True))\n",
    "                    \n",
    "                    # Force one column, even if empty\n",
    "                    if X_feature.shape[1] == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(X_feature, y, test_size=0.2, random_state=42)\n",
    "                    \n",
    "                    # if is_continuous:  # Regression\n",
    "                    #     model = LinearRegression()\n",
    "                    #     model.fit(X_train_feat, y_train_feat)\n",
    "                    #     y_pred = model.predict(X_test_feat)\n",
    "                    #     score = r2_score(y_test_feat, y_pred)  # R² for regression\n",
    "                    #     metric = \"Score\"\n",
    "                        \n",
    "                    # else:  # Classification\n",
    "                    model = LogisticRegression(max_iter=200)  # or DecisionTreeClassifier()\n",
    "                    model.fit(X_train_feat, y_train_feat)\n",
    "                    y_pred = model.predict(X_test_feat)\n",
    "                    score = accuracy_score(y_test_feat, y_pred)\n",
    "                    metric = \"Score\"\n",
    "                        \n",
    "                    feature_performance.append({\"Feature name\": feature, metric: score})\n",
    "                    \n",
    "                feature_performance_df = pd.DataFrame(feature_performance).sort_values(by=metric, ascending=False)\n",
    "                filename = f'data/generated_data/feature_rankings_{target}_{year}_{office}.csv'\n",
    "                feature_performance_df.to_csv(filename, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62b763-a523-4484-aa76-bf7b599dc389",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLDOUT_YEAR = '2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1d543-a8de-44f4-9bc4-69cbbe1f97b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AGGREGATE TRAIN/TEST\n",
    "# Take several years and aggregate into a larger single dataset\n",
    "# for maximum training data, perhaps leaving out one holdout set.\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(f'Num. of offices to process: {len(ELECTIONS)}')\n",
    "\n",
    "for key, value in ELECTIONS.items():\n",
    "    print(f'Num. of years to process: {len(value)}')\n",
    "\n",
    "    OFFICES = [key]\n",
    "    YEARS = value.copy()\n",
    "\n",
    "    if HOLDOUT_YEAR in YEARS:\n",
    "        YEARS.remove(HOLDOUT_YEAR)\n",
    "\n",
    "    if not YEARS:\n",
    "        print(f\"Skipping {key} — no years left after removing holdout.\")\n",
    "        continue\n",
    "\n",
    "    print(f'Process office(s): {key} for year(s): {', '.join(YEARS)}')\n",
    "\n",
    "    df_datasets = makeDatasets(YEARS, OFFICES)\n",
    "    df = aggDatasets(df_datasets, YEARS, OFFICES)\n",
    "\n",
    "    for target in TARGETS:\n",
    "        print(f'Processing target {target}')\n",
    "    \n",
    "        # If features have been ranked, use only those features ranked\n",
    "        # high for the target in quest.\n",
    "        if FEATURES_ALREADY_RANKED:\n",
    "            print(f'Features already ranked, select top...')\n",
    "            feature_importance_file = f'data/generated_data/df_importances_{target}.csv'\n",
    "            top_feature_columns = pd.read_csv(feature_importance_file)['Feature name'].head(TOP_N_FEATURES).tolist()\n",
    "            required_columns = ['standardized_id_num', target]\n",
    "            selected_columns = [col for col in top_feature_columns + required_columns if col in df.columns]\n",
    "            df = df[selected_columns].dropna(subset=[target])\n",
    "        \n",
    "        # Ensure clean 13-character left-padded ID\n",
    "        df['standardized_id_num'] = df['standardized_id_num'].astype(str).str.zfill(13)\n",
    "        \n",
    "        # Drop rows w/o targets\n",
    "        df = df.dropna(subset=[target])\n",
    "        \n",
    "        # Save standardized IDs\n",
    "        X_stids = df['standardized_id_num']\n",
    "        \n",
    "        # Define y and encode\n",
    "        y = df[target]\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y)\n",
    "        \n",
    "        # Drop unneeded columns after saving id\n",
    "        cols_to_drop = [col for col in drop_features if col in df.columns]\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "        X = df.drop(columns=[target])\n",
    "        \n",
    "        # Define categorical and numeric columns for formatting.\n",
    "        categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            X[col] = X[col].astype(str)\n",
    "        \n",
    "        categorical_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "            ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        numeric_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='mean')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        preprocessor = ColumnTransformer([\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "            ('num', numeric_transformer, numeric_cols)\n",
    "        ])\n",
    "        \n",
    "        xgb_classifier = XGBClassifier(\n",
    "            objective=\"multi:softmax\",  # multi:softmax or multi:softprob\n",
    "            num_class=len(y.unique()),\n",
    "            use_label_encoder=False,\n",
    "            n_estimators=200,\n",
    "            max_depth=5,\n",
    "            learning_rate=0.1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            eval_metric=\"mlogloss\"\n",
    "        )\n",
    "        \n",
    "        xgb_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', xgb_classifier)\n",
    "        ])\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, stratify=y, random_state=42)\n",
    "        \n",
    "        xgb_pipeline.fit(X_train, y_train)\n",
    "        y_pred = xgb_pipeline.predict(X_test)\n",
    "        \n",
    "        decoded_y_test = label_encoder.inverse_transform(y_test)\n",
    "        decoded_y_pred = label_encoder.inverse_transform(y_pred)\n",
    "        \n",
    "        # Print classification report\n",
    "        class_report = classification_report(decoded_y_test, decoded_y_pred, output_dict=True)\n",
    "        print(class_report)\n",
    "        \n",
    "        df_class_report = pd.DataFrame(class_report).transpose()\n",
    "        df_class_report = df_class_report.round(3)\n",
    "        with open(f'output/reports/classification_report_{target}_aggregate_classification.md', 'w') as f:\n",
    "            f.write(\"# Classification Report\\n\\n\")\n",
    "            f.write(df_class_report.to_markdown())\n",
    "        \n",
    "        # Create the confusion matrix\n",
    "        cm = confusion_matrix(decoded_y_test, decoded_y_pred)\n",
    "        \n",
    "        # Plot matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                    xticklabels=label_encoder.classes_, \n",
    "                    yticklabels=label_encoder.classes_)\n",
    "        \n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'output/figures/confusion_matrix_{target}_aggregate.png')\n",
    "        plt.close()\n",
    "        # plt.show()\n",
    "        \n",
    "        # Save predictions to disk\n",
    "        results_df = pd.DataFrame({\n",
    "            'standardized_id_num': X_stids.loc[X_test.index],\n",
    "            'true_label': decoded_y_test,\n",
    "            'predicted_label': decoded_y_pred\n",
    "        })\n",
    "        results_df['standardized_id_num'] = results_df['standardized_id_num'].astype(str).str.zfill(13)\n",
    "        \n",
    "        filename = f'data/generated_data/prediction_results_{target}_aggregate_classification.csv'\n",
    "        results_df.to_csv(filename, index=False)\n",
    "        \n",
    "        \n",
    "        #############################\n",
    "        # FEATURE PERFORMANCE\n",
    "        #############################\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        \n",
    "        feature_performance = []\n",
    "        \n",
    "        for feature in tqdm(X.columns):\n",
    "            X_feature = X[[feature]].copy()\n",
    "            \n",
    "            # Handle missing values\n",
    "            if X_feature[feature].dtype == 'object':\n",
    "                X_feature = X_feature.fillna(X_feature.mode().iloc[0])\n",
    "                X_feature = pd.get_dummies(X_feature, drop_first=True)\n",
    "            \n",
    "                # Need at least 1 column after one-hot encoding\n",
    "                if X_feature.shape[1] == 0:\n",
    "                    continue  # Leave as is\n",
    "            else:\n",
    "                X_feature = X_feature.fillna(X_feature.mean(numeric_only=True))\n",
    "            \n",
    "            # Force one column, even if empty\n",
    "            if X_feature.shape[1] == 0:\n",
    "                continue\n",
    "            \n",
    "            X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(X_feature, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            model = LogisticRegression(max_iter=200)  # or DecisionTreeClassifier()\n",
    "            model.fit(X_train_feat, y_train_feat)\n",
    "            y_pred = model.predict(X_test_feat)\n",
    "            score = accuracy_score(y_test_feat, y_pred)\n",
    "            metric = \"Score\"\n",
    "                \n",
    "            feature_performance.append({\"Feature name\": feature, metric: score})\n",
    "            \n",
    "        feature_performance_df = pd.DataFrame(feature_performance).sort_values(by=metric, ascending=False)\n",
    "        filename = f'data/generated_data/feature_rankings_{target}_aggregate_classification.csv'\n",
    "        feature_performance_df.to_csv(filename, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5407f885-f8fc-4405-90ad-27f7e3bf3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHoldoutDataset(holdout_year=HOLDOUT_YEAR):\n",
    "    df_holdout_dataset_list = []\n",
    "    \n",
    "    for office, years in ELECTIONS.items():\n",
    "        for year in years:\n",
    "            if year == holdout_year:\n",
    "                office = office.replace(' ', '_').replace('.', '')\n",
    "                df = pd.read_csv('data/generated_data/07_ml_features_' + year + '_' + office + '.csv', low_memory=False)\n",
    "                df_holdout_dataset_list.append(df)\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    df_holdout = pd.concat(df_holdout_dataset_list, axis=0, ignore_index=True)\n",
    "\n",
    "    return df_holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462bc61-62c0-47d0-81fd-5d5a07842fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benchmark = getHoldoutDataset(HOLDOUT_YEAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a459ce-613b-42b3-b67f-81938390cedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RULE-OF-THUMB BENCHMARKING\n",
    "\n",
    "for target in TARGETS:\n",
    "    \n",
    "    RULE_OF_THUMB_FEATURES = ['partisanship_lean_prev']\n",
    "    \n",
    "    # Check and compute average rule-of-thumb if both fields exist\n",
    "    rule_cols = RULE_OF_THUMB_FEATURES\n",
    "    available_rule_cols = [col for col in rule_cols if col in df_benchmark.columns]\n",
    "    \n",
    "    if len(available_rule_cols) == 0:\n",
    "        raise Exception(\"No rule-of-thumb columns found.\")\n",
    "    \n",
    "    # Calculate rule-of-thumb prediction (average if 2, fallback to 1)\n",
    "    if len(available_rule_cols) == 2:\n",
    "        df_benchmark['rule_of_thumb'] = df_benchmark[available_rule_cols].mean(axis=1)\n",
    "    else:\n",
    "        df_benchmark['rule_of_thumb'] = df_benchmark[available_rule_cols[0]]\n",
    "    \n",
    "    # Round and convert benchmark to original categorical labels\n",
    "    df_benchmark['rule_of_thumb_rounded'] = df_benchmark['rule_of_thumb'].round()\n",
    "    \n",
    "    # Filter to rows with non-null target and benchmark\n",
    "    mask = df_benchmark['rule_of_thumb_rounded'].notna() & df_benchmark[target].notna()\n",
    "    y_true = df_benchmark.loc[mask, target]\n",
    "    y_benchmark = df_benchmark.loc[mask, 'rule_of_thumb_rounded']\n",
    "    \n",
    "    # Fit encoder on all labels if needed\n",
    "    label_encoder.fit(df_benchmark[target].dropna().astype(str).unique())\n",
    "    \n",
    "    # Encode labels\n",
    "    y_true_encoded = label_encoder.transform(y_true.astype(str))\n",
    "    y_benchmark_encoded = label_encoder.transform(y_benchmark.astype(str))\n",
    "    \n",
    "    # print(\"\\nRule-of-Thumb Benchmark (Average of Previous Values):\")\n",
    "    # print(classification_report(y_true_encoded, y_benchmark_encoded, target_names=label_encoder.classes_))\n",
    "    decoded_y_true = label_encoder.inverse_transform(y_true_encoded)\n",
    "    decoded_y_benchmark = label_encoder.inverse_transform(y_benchmark_encoded)\n",
    "    \n",
    "    print(classification_report(decoded_y_true, decoded_y_benchmark))\n",
    "    \n",
    "    # Create the confusion matrix\n",
    "    cm = confusion_matrix(decoded_y_true, decoded_y_benchmark)\n",
    "    \n",
    "    # Plot with seaborn for nice formatting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_encoder.classes_, \n",
    "                yticklabels=label_encoder.classes_)\n",
    "    \n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output/figures/confusion_matrix_{target}_aggregate_benchmark.png')\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "    \n",
    "    # Save benchmark results\n",
    "    df_benchmark_results = pd.DataFrame({\n",
    "        'standardized_id_num': df_benchmark.loc[mask, 'standardized_id_num'],\n",
    "        'true_label': y_true,\n",
    "        'benchmark_label': y_benchmark\n",
    "    })\n",
    "    \n",
    "    filename = f\"data/generated_data/benchmark_results_{target}_aggregate.csv\"\n",
    "    df_benchmark_results.to_csv(filename, index=False)\n",
    "    print(f\"Saved benchmark predictions to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca09dd-e52b-4849-84f8-e2c9f858d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict targets using holdout – U.S. House only\n",
    "df_holdout = getHoldoutDataset(HOLDOUT_YEAR)\n",
    "\n",
    "for target in TARGETS:\n",
    "    years = [HOLDOUT_YEAR]\n",
    "    offices = ['U.S. House']\n",
    "    \n",
    "    df_holdout_benchmark = df_holdout.copy()\n",
    "    \n",
    "    if FEATURES_ALREADY_RANKED:\n",
    "        print(f'Features already ranked, select top...')\n",
    "        feature_importance_file = f'data/generated_data/df_importances_{target}.csv'\n",
    "        top_feature_columns = pd.read_csv(feature_importance_file)['Feature name'].head(TOP_N_FEATURES).tolist()\n",
    "        required_columns = ['standardized_id_num', target]\n",
    "        selected_columns = [col for col in top_feature_columns + required_columns if col in df_holdout.columns]\n",
    "        df_holdout = df_holdout[selected_columns].dropna(subset=[target])\n",
    "    \n",
    "    # Clean ID\n",
    "    df_holdout['standardized_id_num'] = df_holdout['standardized_id_num'].astype(str).str.zfill(13)\n",
    "    \n",
    "    # Drop rows w/o targets\n",
    "    df_holdout = df_holdout.dropna(subset=[target])\n",
    "    \n",
    "    # Save standardized IDs\n",
    "    X_stids = df_holdout['standardized_id_num']\n",
    "    \n",
    "    # Define y and encode\n",
    "    y = df_holdout[target]\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Drop columns after saving id\n",
    "    cols_to_drop = [col for col in drop_features if col in df_holdout.columns]\n",
    "    df_holdout = df_holdout.drop(columns=cols_to_drop)\n",
    "    X = df_holdout.drop(columns=[target])\n",
    "    \n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numeric_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        X[col] = X[col].astype(str)\n",
    "    \n",
    "    print(\"Predicting on 2024 holdout set...\")\n",
    "    y_pred_encoded = xgb_pipeline.predict(X)\n",
    "    \n",
    "    decoded_y_test = label_encoder.inverse_transform(y_encoded)\n",
    "    decoded_y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "    \n",
    "    class_report = classification_report(decoded_y_test, decoded_y_pred, output_dict=True)\n",
    "    print(classification_report(y_true_encoded, y_benchmark_encoded, target_names=label_encoder.classes_))\n",
    "\n",
    "    df_class_report = pd.DataFrame(class_report).transpose()\n",
    "    df_class_report = df_class_report.round(3)\n",
    "    with open(f'output/reports/classification_report_{target}_holdout.md', 'w') as f:\n",
    "        f.write(\"# Classification Report\\n\\n\")\n",
    "        f.write(df_class_report.to_markdown())\n",
    "    \n",
    "    cm = confusion_matrix(decoded_y_test, decoded_y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_encoder.classes_, \n",
    "                yticklabels=label_encoder.classes_)\n",
    "    \n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'output/figures/confusion_matrix_{target}_holdout.png')\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "    \n",
    "    # Save predictions to disk.\n",
    "    results_df = pd.DataFrame({\n",
    "        'standardized_id_num': X_stids.reset_index(drop=True),\n",
    "        'true_label': decoded_y_test,\n",
    "        'predicted_label': decoded_y_pred\n",
    "    })\n",
    "    results_df['standardized_id_num'] = results_df['standardized_id_num'].astype(str).str.zfill(13)\n",
    "    \n",
    "    filename = f'data/generated_data/predicted_{target}_{HOLDOUT_YEAR}_holdout.csv'\n",
    "    results_df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5259b-26b8-4e5a-b023-8dba8ac70e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate benchmark of trained predictor from holdout – U.S. House only\n",
    "for target in TARGETS:\n",
    "    years = [HOLDOUT_YEAR]\n",
    "    offices = ['U.S. House']\n",
    "\n",
    "    df_holdout_benchmark = df_holdout.copy()\n",
    "\n",
    "    # INDIVIDUAL BENCHMARKING\n",
    "    RULE_OF_THUMB_FEATURES = ['partisanship_lean_prev']\n",
    "    \n",
    "    # Check and compute average rule-of-thumb if both fields exist\n",
    "    # rule_cols = ['partisanship_lean_prev', 'partisanship_lean_prev_prev']\n",
    "    rule_cols = RULE_OF_THUMB_FEATURES\n",
    "    available_rule_cols = [col for col in rule_cols if col in df_holdout_benchmark.columns]\n",
    "    \n",
    "    if len(available_rule_cols) == 0:\n",
    "        raise Exception(\"No rule-of-thumb columns found.\")\n",
    "    \n",
    "    # Calculate rule-of-thumb prediction (average if 2, fallback to 1)\n",
    "    if len(available_rule_cols) == 2:\n",
    "        df_holdout_benchmark['rule_of_thumb'] = df_holdout_benchmark[available_rule_cols].mean(axis=1)\n",
    "    else:\n",
    "        df_holdout_benchmark['rule_of_thumb'] = df_holdout_benchmark[available_rule_cols[0]]\n",
    "    \n",
    "    # Round and convert benchmark to original categorical labels\n",
    "    df_holdout_benchmark['rule_of_thumb_rounded'] = df_holdout_benchmark['rule_of_thumb'].round()\n",
    "    \n",
    "    # Filter to rows with non-null target and benchmark\n",
    "    mask = df_holdout_benchmark['rule_of_thumb_rounded'].notna() & df_holdout_benchmark[target].notna()\n",
    "    y_true = df_holdout_benchmark.loc[mask, target]\n",
    "    y_benchmark = df_holdout_benchmark.loc[mask, 'rule_of_thumb_rounded']\n",
    "    \n",
    "    # Fit encoder on all labels if needed\n",
    "    label_encoder.fit(df_holdout_benchmark[target].dropna().astype(str).unique())\n",
    "    \n",
    "    # Encode labels\n",
    "    y_true_encoded = label_encoder.transform(y_true.astype(str))\n",
    "    y_benchmark_encoded = label_encoder.transform(y_benchmark.astype(str))\n",
    "    \n",
    "    print(\"\\nRule-of-Thumb Benchmark (Average of Previous Values):\")\n",
    "    print(classification_report(y_true_encoded, y_benchmark_encoded, target_names=label_encoder.classes_))\n",
    "    \n",
    "    # Save benchmark results\n",
    "    benchmark_df = pd.DataFrame({\n",
    "        'standardized_id_num': df_holdout_benchmark.loc[mask, 'standardized_id_num'],\n",
    "        'true_label': y_true,\n",
    "        'benchmark_label': y_benchmark\n",
    "    })\n",
    "    \n",
    "    filename = f\"data/generated_data/benchmark_results_{target}_{year}_{office}_classification.csv\"\n",
    "    benchmark_df.to_csv(filename, index=False)\n",
    "    print(f\"Saved benchmark predictions to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db84598-c8de-4e3d-886c-176b291ec96b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
