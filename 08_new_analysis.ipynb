{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8441d4f-063b-456e-ba91-0fdeab3ff446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and parse GeoDataFrame\n",
    "df = pd.read_csv('merged.csv', low_memory=False)\n",
    "\n",
    "# Parse geometry from WKT (if needed)\n",
    "if df['geometry'].dtype == object:\n",
    "    df['geometry'] = df['geometry'].apply(wkt.loads)\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "gdf.set_crs(epsg=4326, inplace=True)\n",
    "\n",
    "# Extract centroid coordinates (in projected CRS for clustering)\n",
    "gdf_proj = gdf.to_crs(epsg=6493)  # EPSG 6493 = NAD83 / Michigan Central (meters)\n",
    "gdf['centroid_x'] = gdf_proj.geometry.centroid.x\n",
    "gdf['centroid_y'] = gdf_proj.geometry.centroid.y\n",
    "\n",
    "# Select relevant features for clustering\n",
    "# Drop obvious non-features (like IDs), add centroids for spatial awareness\n",
    "ignore_cols = ['geometry', 'standardized_id_num', 'partisan_temp_category']\n",
    "feature_cols = [col for col in df.columns if col not in ignore_cols and pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "# Parameters\n",
    "geo_weight = 90  # Try 10–100; higher = stronger spatial grouping\n",
    "\n",
    "# Prepare features\n",
    "features = gdf[feature_cols + ['centroid_x', 'centroid_y']].copy()\n",
    "\n",
    "# Clean feature set\n",
    "features = features.apply(pd.to_numeric, errors='coerce')\n",
    "features = features.dropna(axis=1, how='all')\n",
    "features = features.fillna(features.mean(numeric_only=True))\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Identify columns\n",
    "spatial_index_x = features.columns.get_loc('centroid_x')\n",
    "spatial_index_y = features.columns.get_loc('centroid_y')\n",
    "\n",
    "# Re-weight spatial coordinates post-scaling\n",
    "X_scaled[:, spatial_index_x] *= geo_weight\n",
    "X_scaled[:, spatial_index_y] *= geo_weight\n",
    "\n",
    "# Remove old cluster column if present\n",
    "if 'cluster' in gdf.columns:\n",
    "    gdf = gdf.drop(columns='cluster')\n",
    "\n",
    "# Cluster\n",
    "k = 30\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "gdf['cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Plot (very basic plot, can improve)\n",
    "gdf.plot(column='cluster', legend=True, figsize=(10, 6), cmap='tab10')\n",
    "plt.title(\"Clustered Voting Precincts\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6462947-5221-4a8a-9b6d-15f469233466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeCensusFeatureLabels(feature_name, year):\n",
    "    # We need to find the directory by only knowing the first part\n",
    "    # of the name, which is the census id/code.\n",
    "    if year == 2024: # No 2024 data yet.\n",
    "        year = 2023\n",
    "    feature_name_code = ''\n",
    "    if feature_name[:1] == 'S':\n",
    "        feature_name_code = feature_name[:5].upper()\n",
    "        feature_name_label = feature_name[6:]\n",
    "        data_type = 'ACSST5Y'\n",
    "    elif feature_name[:1] == 'B':\n",
    "        feature_name_code = feature_name[:6].upper()\n",
    "        feature_name_label = feature_name[7:]\n",
    "        data_type = 'ACSDT5Y'\n",
    "    elif feature_name[:1] == 'D':\n",
    "        feature_name_code = feature_name[:4].upper()\n",
    "        feature_name_label = feature_name[5:]\n",
    "        data_type = 'ACSDP5Y'\n",
    "    else:\n",
    "        # Not a census feature\n",
    "        return feature_name\n",
    "\n",
    "    partial_dir = feature_name_code.lower()\n",
    "    base_path = 'data/census/'\n",
    "    \n",
    "    matching_dir = glob.glob(os.path.join(base_path, partial_dir + '*'))\n",
    "    \n",
    "    if matching_dir:\n",
    "        target_dir = matching_dir[0]\n",
    "        \n",
    "        dataset_name = after_underscore = target_dir.split(\"_\", 1)[1] # characters following the code.\n",
    "        dataset_name = dataset_name.replace('_', ' ').title()\n",
    "        file_path = os.path.join(target_dir, f'{data_type}{year}.{feature_name_code}-Column-Metadata.csv')\n",
    "\n",
    "        df_columns = pd.read_csv(file_path)\n",
    "        label = df_columns[df_columns['Column Name'] == feature_name].values[0][1]\n",
    "        parts = label.split('!!')\n",
    "        short_label = ' | '.join(parts)\n",
    "        feature_label = f'{feature_name} | {dataset_name} | {short_label}'\n",
    "\n",
    "        return feature_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1cb861-b241-4e9c-8ead-559eaffa1fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512b6aaa-c93f-4b94-a23b-bbd544ad41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# List of fields to exclude from analysis\n",
    "exclude_cols = [\n",
    "    'Michigan County Code', 'City/Township Code', 'Ward Number', 'Precinct Number', 'Precinct Label', \n",
    "    'Office Code', 'District Code', 'Status Code', 'Office Description', 'County Name', 'Census County Code', \n",
    "    'Election Year', 'Election Type', 'City/Township Description', \n",
    "    'standardized_id', 'standardized_id_num', \n",
    "    'dem_votes', 'oth_votes', 'rep_votes', \n",
    "    'total_votes', \n",
    "    'dem_share', 'rep_share', 'oth_share', \n",
    "    'partisan_temp', \n",
    "    'registered_voters', 'turnout_pct', \n",
    "    # 'dem_share_prev', 'rep_share_prev', 'oth_share_prev', \n",
    "    # 'dem_share_change_prev', 'rep_share_change_prev', 'oth_share_change_prev', \n",
    "    'dem_share_change_curr', 'rep_share_change_curr', 'oth_share_change_curr', \n",
    "    # 'dem_votes_change_prev', 'rep_votes_change_prev', 'oth_votes_change_prev', \n",
    "    'dem_votes_change_curr', 'rep_votes_change_curr', 'oth_votes_change_curr', \n",
    "    # 'registered_voters_change_prev', 'turnout_pct_change_prev', \n",
    "    'registered_voters_change_curr', 'turnout_pct_change_curr', \n",
    "    # 'partisan_temp_prev', 'partisan_temp_change_prev', \n",
    "    'partisan_temp_change_curr', \n",
    "    'nearest_bound_school_district', 'nearest_bound_census_tract', 'nearest_bound_zipcode', \n",
    "    'geometry', 'tractce_tract', 'geoid_tract', 'geoidfq_tract', \n",
    "    'name_tract', 'aland_tract', 'awater_tract', 'geometry_tract', \n",
    "    'true_label', 'predicted_label',\n",
    "]\n",
    "\n",
    "# List of excluded fields generated by clustering.\n",
    "exclude_added_cols = [\n",
    "    'latitude', 'longitude', 'centroid_x', 'centroid_y',\n",
    "]\n",
    "\n",
    "exclude_cols.extend(exclude_added_cols)\n",
    "\n",
    "# Use the same features used for clustering (excluding spatial + excluded fields)\n",
    "cluster_features = features.drop(columns=[col for col in exclude_cols if col in features.columns], errors='ignore')\n",
    "feature_names = cluster_features.columns\n",
    "\n",
    "# Combine cluster labels with features\n",
    "clustered = pd.concat([gdf[['cluster']], cluster_features], axis=1)\n",
    "\n",
    "# Overall mean and std for z-score reference\n",
    "global_mean = cluster_features.mean()\n",
    "global_std = cluster_features.std()\n",
    "\n",
    "# Dictionary to hold top features per cluster\n",
    "cluster_summary = {}\n",
    "\n",
    "top_features_names = []\n",
    "\n",
    "for cluster_id in sorted(clustered['cluster'].unique()):\n",
    "    cluster_data = clustered[clustered['cluster'] == cluster_id]\n",
    "    cluster_mean = cluster_data.mean(numeric_only=True)\n",
    "\n",
    "    # z-score: how different is this feature in this cluster from the overall mean\n",
    "    z_scores = (cluster_mean - global_mean) / global_std\n",
    "\n",
    "    # Get top N defining features (by absolute deviation)\n",
    "    top_features = z_scores.abs().sort_values(ascending=False).head(100)\n",
    "    top_features = top_features.index.tolist()\n",
    "    labeled_features = [makeCensusFeatureLabels(str(feature_name), year) for feature_name in top_features]\n",
    "    \n",
    "    cluster_summary[cluster_id] = {\n",
    "        'top_features': labeled_features,\n",
    "        'z_scores': z_scores[top_features].round(3).to_dict(),\n",
    "    }\n",
    "\n",
    "# Store summaries\n",
    "cluster_dfs = {}\n",
    "for cluster_id, info in cluster_summary.items():\n",
    "    cluster_rows = []\n",
    "\n",
    "    # Get coded feature names\n",
    "    feature_codes = list(info['z_scores'].keys())\n",
    "\n",
    "    # Load feature labels\n",
    "    labeled_features = [makeCensusFeatureLabels(str(code), year) for code in feature_codes]\n",
    "\n",
    "    for code, label in zip(feature_codes, labeled_features):\n",
    "        z = info['z_scores'].get(code, None)\n",
    "        cluster_rows.append({\n",
    "            'feature_code': code,\n",
    "            'feature_label': label,\n",
    "            'z_score': z\n",
    "        })\n",
    "\n",
    "    cluster_dfs[cluster_id] = pd.DataFrame(cluster_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3acbcc-250f-40f4-b8ce-a49793b1ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cluster_id, df in cluster_dfs.items():\n",
    "#     print(f\"\\nCluster {cluster_id} — Top 10 Features:\")\n",
    "    \n",
    "#     # Sort by absolute z-score descending, just in case it's not sorted already\n",
    "#     top10 = df.sort_values(by='z_score', key=abs, ascending=False).head(10)\n",
    "    \n",
    "#     for i, row in top10.iterrows():\n",
    "#         print(f\"  {row['feature_label']} (z = {row['z_score']:.2f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b21b2-c8f9-416c-afeb-427bd7ab1d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_dfs = {}\n",
    "\n",
    "for cluster_id, df in cluster_dfs.items():\n",
    "    # Sort by absolute z-score and select top 10 rows\n",
    "    top10 = df.sort_values(by='z_score', key=abs, ascending=False).head(10)\n",
    "    \n",
    "    # Keep only the feature_label column (or others if desired)\n",
    "    top10_dfs[cluster_id] = top10[['feature_label', 'z_score']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec18b5-288c-468b-8b2a-c3a49a31e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc755fa-1bde-4945-b09e-ccc166090683",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_dfs[0].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d3c0b-66d4-4233-92bc-33fffc97c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Combine all cluster DataFrames into one\n",
    "combined_df = pd.concat(cluster_dfs.values(), ignore_index=True)\n",
    "\n",
    "# 2. Count how many clusters each feature_label appears in\n",
    "label_counts = combined_df['feature_label'].value_counts()\n",
    "\n",
    "# 3. Only keep labels that appear in *all* clusters\n",
    "min_cluster_count = 5  # or len(cluster_dfs)\n",
    "common_labels = label_counts[label_counts >= min_cluster_count].index.tolist()\n",
    "\n",
    "# 4. Filter to only common features\n",
    "common_df = combined_df[combined_df['feature_label'].isin(common_labels)]\n",
    "\n",
    "# 5. Group and average z_scores\n",
    "summary_df = (\n",
    "    common_df.groupby('feature_label')['z_score']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# 6. Order by absolute z_score magnitude (descending)\n",
    "summary_df['abs_z'] = summary_df['z_score'].abs()\n",
    "summary_df = summary_df.sort_values(by='abs_z', ascending=False).drop(columns='abs_z')\n",
    "\n",
    "# Done!\n",
    "print(summary_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555784c5-238a-4193-bae8-84a882236239",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cluster_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8493d8d1-a819-4926-80ab-9ba1ed1850c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
