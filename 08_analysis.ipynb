{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e82e2-9b95-43d5-b1e1-2502659316b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import glob\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numbers\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap, to_rgb\n",
    "from matplotlib.legend_handler import HandlerPatch\n",
    "from matplotlib.patches import PathPatch\n",
    "from matplotlib.path import Path as MplPath\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from shapely import wkt\n",
    "from shapely.geometry import box, Point\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f6fe4-8dde-4c4d-993c-bec401984823",
   "metadata": {},
   "source": [
    "#### OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5916f5d-bb7d-48bb-9552-a99fa783f058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "file = open(\"OpenAI.key\", \"r\")\n",
    "key = file.read()\n",
    "\n",
    "OPENAI_API_KEY = key\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6254018-ddb3-4b37-a0ab-d687e5854f08",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc3cb3-7ab7-4eda-a0c5-9848cc116f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLM_ACTIVE = False\n",
    "\n",
    "YEAR = '2022'\n",
    "\n",
    "census_datasets = [\n",
    "    'b02001_race', 'b04007_ancestry', 'b05012_nativity_us', 'b08303_travel_time_work', \n",
    "    'b25003_housing_rentership', 'dp04_housing_characteristics', 'dp05_age_race', 's0101_age_sex', \n",
    "    's1101_households_families', 's1201_marital_status', 's1501_educational_attainment', 's1701_income_poverty', \n",
    "    's1903_median_income', 's2101_veteran_status', 's2201_food_stamps', 's2301_employment_status', \n",
    "    's2401_occupation_sex', 's2403_industry_sex', 's2501_occupancy_characteristics', \n",
    "    's2503_financial_characteristics', 's2701_health_insurance',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92671dd2-dec4-4980-906f-e7fdb601a963",
   "metadata": {},
   "source": [
    "### Primary Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeace1f-e120-4c09-8231-a9e709150cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_precinct_data(year, office):\n",
    "    office_filename = office.replace(\".\", \"\").replace(\" \", \"_\")\n",
    "    print(\"Loading precinct data...\")\n",
    "    precinct_data = pd.read_csv(f'data/generated_data/df_06_tract_{year}_{office_filename}.csv')\n",
    "    predictions = pd.read_csv(f'data/generated_data/predicted_partisanship_change_{year}_{office_filename}.csv')\n",
    "    precinct_data = precinct_data.merge(predictions, on='standardized_id_num', how='left')\n",
    "    return predictions, precinct_data\n",
    "\n",
    "\n",
    "def load_census_data(census_datasets, year, office):\n",
    "    print(\"Loading census data...\")\n",
    "    census_dataset_dfs = {}\n",
    "    office_filename = office.replace(\".\", \"\").replace(\" \", \"_\")\n",
    "    \n",
    "    for census_dataset in census_datasets:\n",
    "        census_dataset = census_dataset.lower()\n",
    "        if census_dataset.startswith('s'):\n",
    "            census_dataset_code = census_dataset[:5].upper()\n",
    "            census_dataset_label = census_dataset[6:]\n",
    "            data_type = 'ACSST5Y'\n",
    "        elif census_dataset.startswith('b'):\n",
    "            census_dataset_code = census_dataset[:6].upper()\n",
    "            census_dataset_label = census_dataset[7:]\n",
    "            data_type = 'ACSDT5Y'\n",
    "        elif census_dataset.startswith('d'):\n",
    "            census_dataset_code = census_dataset[:4].upper()\n",
    "            census_dataset_label = census_dataset[5:]\n",
    "            data_type = 'ACSDP5Y'\n",
    "        \n",
    "        df = pd.read_csv(f'data/generated_data/df_06_{census_dataset_label}_{year}_{office_filename}.csv')\n",
    "        df.rename(columns={f'geoid_{census_dataset_label}': 'geoidfq_tract'}, inplace=True)\n",
    "        census_dataset_dfs[census_dataset_label] = df\n",
    "\n",
    "    return census_dataset_dfs\n",
    "\n",
    "\n",
    "def merge_datasets(precinct_data, census_dataset_dfs):\n",
    "    print(\"Merging datasets...\")\n",
    "    merged_df = precinct_data.copy()\n",
    "    \n",
    "    for key, df_census in census_dataset_dfs.items():\n",
    "        df_census_copy = df_census.copy()\n",
    "        df_census_copy.rename(columns={f'geoid_{key}': 'geoidfq_tract'}, inplace=True)\n",
    "        merged_df = merged_df.merge(df_census_copy, on='geoidfq_tract', how='left')\n",
    "    \n",
    "    print(f\"Merged dataset has {merged_df.shape[1]} columns\")\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c075fdf-a1e3-47e5-a8dc-a8a2e90e93a6",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81bb2bb-cd3d-47f8-8543-99eb9920a419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_voting_patterns(merged_df ):\n",
    "    print(\"Analyzing voting patterns by cluster...\")\n",
    "    \n",
    "    if 'partisanship' not in merged_df.columns:\n",
    "        try:\n",
    "            merged_df['partisanship'] = merged_df.apply(categorize_partisanship, axis=1)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not compute partisanship: {e}\")\n",
    "    \n",
    "    if 'partisanship_change' not in merged_df.columns:\n",
    "        try:\n",
    "            merged_df['partisanship_change'] = merged_df.apply(categorize_partisan_change, axis=1)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not compute partisanship_change: {e}\")\n",
    "    \n",
    "    def safe_mode(x):\n",
    "        try:\n",
    "            return x.mode().iloc[0] if not x.empty else 'unknown'\n",
    "        except:\n",
    "            return 'unknown'\n",
    "    \n",
    "    agg_dict = {'standardized_id_num': 'count'}\n",
    "    \n",
    "    for col in ['dem_share_prev', 'rep_share_prev', 'dem_share_change', 'rep_share_change', \n",
    "               'partisanship', 'partisanship_change']:\n",
    "        if col in merged_df.columns:\n",
    "            if col in ['partisanship', 'partisanship_change']:\n",
    "                agg_dict[col] = safe_mode\n",
    "            else:\n",
    "                agg_dict[col] = 'mean'\n",
    "    \n",
    "    cluster_analysis = merged_df.groupby('cluster').agg(agg_dict).reset_index()\n",
    "\n",
    "    # Extract per cluster\n",
    "    standardized_id_nums = (\n",
    "        merged_df.groupby('cluster')['standardized_id_num']\n",
    "        .apply(list)\n",
    "        .reset_index()\n",
    "        .rename(columns={'standardized_id_num': 'standardized_id_nums'})\n",
    "    )\n",
    "    cluster_analysis = cluster_analysis.merge(standardized_id_nums, on='cluster')\n",
    "    \n",
    "    return cluster_analysis\n",
    "\n",
    "\n",
    "def assign_centroid_latlon(gdf):\n",
    "    gdf_proj = gdf.to_crs(epsg=3857)\n",
    "    gdf_proj['centroid'] = gdf_proj.geometry.centroid\n",
    "    centroids_ll = gdf_proj.set_geometry('centroid').to_crs(epsg=4326)\n",
    "    return centroids_ll.geometry.y, centroids_ll.geometry.x\n",
    "\n",
    "\n",
    "def calculate_cluster_stats(cluster_id, cluster_analysis):\n",
    "    cluster_size = cluster_analysis.loc[cluster_analysis['cluster'] == cluster_id, 'standardized_id_num'].values[0]\n",
    "    if isinstance(cluster_size, numbers.Number) and pd.notnull(cluster_size):\n",
    "        cluster_size = int(cluster_size)\n",
    "        total = cluster_analysis['standardized_id_num'].sum()\n",
    "        cluster_size_percent = (cluster_size / total) * 100 if total else 0\n",
    "    else:\n",
    "        cluster_size = \"Unknown\"\n",
    "        cluster_size_percent = 0\n",
    "\n",
    "    return cluster_size, cluster_size_percent\n",
    "\n",
    "\n",
    "def categorize_partisanship(row):\n",
    "    try:\n",
    "        if row[\"dem_share_prev\"] >= 0.667:\n",
    "            return \"strong democrat\"\n",
    "        elif row[\"dem_share_prev\"] >= 0.501:\n",
    "            return \"leans democrat\"\n",
    "        elif row[\"rep_share_prev\"] >= 0.667:\n",
    "            return \"strong republican\"\n",
    "        elif row[\"rep_share_prev\"] >= 0.501:\n",
    "            return \"leans republican\"\n",
    "        elif row[\"oth_share_prev\"] >= 0.667:\n",
    "            return \"strong independent\"\n",
    "        elif row[\"oth_share_prev\"] >= 0.501:\n",
    "            return \"leans independent\"\n",
    "        else:\n",
    "            return \"neutral\"\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def categorize_partisan_change(row):\n",
    "    try:\n",
    "        change = row[\"rep_share_change\"] - row[\"dem_share_change\"]\n",
    "        \n",
    "        if np.abs(change) >= 0.01:\n",
    "            if change > 0.5:\n",
    "                return \"gargantuanly more republican\"\n",
    "            if change > 0.35:\n",
    "                return \"massively more republican\"\n",
    "            if change > 0.25:\n",
    "                return \"much much more republican\"\n",
    "            if change > 0.15:\n",
    "                return \"much more republican\"\n",
    "            if change > 0.1:\n",
    "                return \"more republican\"\n",
    "            if change > 0.05:\n",
    "                return \"slightly more republican\"\n",
    "            elif change > 0.01:\n",
    "                return \"very slightly more republican\"\n",
    "            elif change > 0.005:\n",
    "                return \"infinitesimally more republican\"\n",
    "            elif change < -0.5:\n",
    "                return \"gargantuanly more democrat\"\n",
    "            elif change < -0.35:\n",
    "                return \"massively more democrat\"\n",
    "            elif change < -0.25:\n",
    "                return \"much much democrat\"\n",
    "            elif change < -0.15:\n",
    "                return \"much more democrat\"\n",
    "            elif change < -0.1:\n",
    "                return \"more democrat\"\n",
    "            elif change < -0.05:\n",
    "                return \"slightly more democrat\"\n",
    "            elif change < -0.01:\n",
    "                return \"very slightly more democrat\"\n",
    "            elif change < -0.005:\n",
    "                return \"infinitesimally more democrat\"\n",
    "        else:\n",
    "            return \"no change\"\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def create_clusters(merged_df, valid_features, n_clusters=5, geo_sensitive=True, geo_weight=1):\n",
    "    print(\"Creating clusters...\")\n",
    "    \n",
    "    X = handle_missing_values(merged_df, valid_features)\n",
    "    if X.empty or len(valid_features) == 0:\n",
    "        print(\"Error: No valid features to cluster.\")\n",
    "        return merged_df, None\n",
    "    \n",
    "    # Must preceed scaler.\n",
    "    if geo_sensitive == True:\n",
    "        merged_df['geometry'] = merged_df['geometry'].apply(lambda g: wkt.loads(g) if isinstance(g, str) else g)\n",
    "        merged_df = gpd.GeoDataFrame(merged_df, geometry='geometry')\n",
    "        merged_df.set_crs(epsg=4326, inplace=True)  # Critical\n",
    "        gdf_projected = merged_df.to_crs(epsg=6493)\n",
    "        centroids = gdf_projected.geometry.centroid\n",
    "        centroids_ll = gpd.GeoSeries(centroids, crs=gdf_projected.crs).to_crs(epsg=4326)\n",
    "        merged_df['longitude'] = centroids_ll.x\n",
    "        merged_df['latitude'] = centroids_ll.y\n",
    "        geo_features = ['longitude', 'latitude']\n",
    "        X[geo_features] = merged_df[geo_features]\n",
    "        valid_features += geo_features\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Must follow scaler.\n",
    "    if geo_sensitive == True:\n",
    "        feature_names = valid_features + geo_features\n",
    "        geo_indices = [feature_names.index(gf) for gf in geo_features]\n",
    "        for idx in geo_indices:\n",
    "            X_scaled[:, idx] *= geo_weight\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    merged_df['cluster'] = cluster_labels\n",
    "    centers = pd.DataFrame(scaler.inverse_transform(kmeans.cluster_centers_), columns=valid_features)\n",
    "    \n",
    "    return merged_df, centers\n",
    "\n",
    "\n",
    "def determine_partisan_base_and_trend(dem_share, rep_share, partisan_shift):\n",
    "    if isinstance(dem_share, (int, float)) and isinstance(rep_share, (int, float)):\n",
    "        partisan_base = \"Democratic-leaning\" if dem_share > rep_share else \"Republican-leaning\"\n",
    "    else:\n",
    "        partisan_base = \"Unknown\"\n",
    "\n",
    "    if isinstance(partisan_shift, (int, float)):\n",
    "        if partisan_shift > 0.01:\n",
    "            trend = \"Shifting Right\"\n",
    "        elif partisan_shift < -0.01:\n",
    "            trend = \"Shifting Left\"\n",
    "        else:\n",
    "            trend = \"Stable\"\n",
    "    else:\n",
    "        trend = \"Unknown\"\n",
    "\n",
    "    return partisan_base, trend\n",
    "\n",
    "\n",
    "def format_shares_and_changes(dem_share, rep_share, dem_change, rep_change):\n",
    "    dem_share_fmt = f\"{dem_share:.1%}\" if isinstance(dem_share, (int, float)) else \"Unknown\"\n",
    "    rep_share_fmt = f\"{rep_share:.1%}\" if isinstance(rep_share, (int, float)) else \"Unknown\"\n",
    "    dem_change_fmt = f\"{dem_change:+.1%}\" if isinstance(dem_change, (int, float)) else \"Unknown\"\n",
    "    rep_change_fmt = f\"{rep_change:+.1%}\" if isinstance(rep_change, (int, float)) else \"Unknown\"\n",
    "\n",
    "    return dem_share_fmt, rep_share_fmt, dem_change_fmt, rep_change_fmt\n",
    "\n",
    "\n",
    "def extract_voting_patterns(cluster_id, cluster_analysis):\n",
    "    dem_share = cluster_analysis.loc[cluster_analysis['cluster'] == cluster_id, 'dem_share_prev'].values[0] if 'dem_share_prev' in cluster_analysis.columns else 0\n",
    "    rep_share = cluster_analysis.loc[cluster_analysis['cluster'] == cluster_id, 'rep_share_prev'].values[0] if 'rep_share_prev' in cluster_analysis.columns else 0\n",
    "    dem_change = cluster_analysis.loc[cluster_analysis['cluster'] == cluster_id, 'dem_share_change'].values[0] if 'dem_share_change' in cluster_analysis.columns else 0\n",
    "    rep_change = cluster_analysis.loc[cluster_analysis['cluster'] == cluster_id, 'rep_share_change'].values[0] if 'rep_share_change' in cluster_analysis.columns else 0\n",
    "    partisan_change_category = cluster_analysis.loc[cluster_analysis['cluster'] == cluster_id, 'partisanship_change'].values[0] if 'partisanship_change' in cluster_analysis.columns else \"unknown\"\n",
    "    partisan_shift = rep_change - dem_change\n",
    "\n",
    "    return dem_share, rep_share, dem_change, rep_change, partisan_change_category, partisan_shift\n",
    "\n",
    "\n",
    "def featuresUsed(features):\n",
    "    print(\"Using these features across all charts:\")\n",
    "    for feat in features:\n",
    "        print(f\"  - {feat}\")\n",
    "\n",
    "\n",
    "def get_defining_features(centers, cluster_id):\n",
    "    try:\n",
    "        other_centers = centers.drop(cluster_id)\n",
    "        differences = centers.iloc[cluster_id] - other_centers.mean()\n",
    "        top_differences = differences.abs().sort_values(ascending=False).head(3)\n",
    "        defining_features = top_differences.index.tolist()\n",
    "    except:\n",
    "        defining_features = []\n",
    "\n",
    "    return defining_features\n",
    "\n",
    "\n",
    "def handle_missing_values(df, features):\n",
    "    print(\"Handling missing values...\")\n",
    "    X = df[features].copy()\n",
    "    X = X.apply(pd.to_numeric, errors='coerce')\n",
    "    X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    all_nan_cols = X.columns[X.isna().all()]\n",
    "    if len(all_nan_cols) > 0:\n",
    "        print(f\"Filling {len(all_nan_cols)} entirely-NaN columns with 0s:\")\n",
    "        for col in all_nan_cols:\n",
    "            print(f\"  - {col}\")\n",
    "        X[all_nan_cols] = 0.0\n",
    "\n",
    "    X.fillna(X.median(numeric_only=True), inplace=True)\n",
    "    \n",
    "    remaining_na = X.isna().sum().sum()\n",
    "    if remaining_na > 0:\n",
    "        print(f\"Warning: {remaining_na} NaNs remain in the feature matrix after cleaning!\")\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "def makeCensusFeatureLabels(feature_name, year):\n",
    "    # We need to find the directory by only knowing the first part\n",
    "    # of the name, which is the census id/code.\n",
    "    if year == 2024: # No 2024 data yet.\n",
    "        year = 2023\n",
    "    feature_name_code = ''\n",
    "    if feature_name[:1] == 'S':\n",
    "        feature_name_code = feature_name[:5].upper()\n",
    "        feature_name_label = feature_name[6:]\n",
    "        data_type = 'ACSST5Y'\n",
    "    elif feature_name[:1] == 'B':\n",
    "        feature_name_code = feature_name[:6].upper()\n",
    "        feature_name_label = feature_name[7:]\n",
    "        data_type = 'ACSDT5Y'\n",
    "    elif feature_name[:1] == 'D':\n",
    "        feature_name_code = feature_name[:4].upper()\n",
    "        feature_name_label = feature_name[5:]\n",
    "        data_type = 'ACSDP5Y'\n",
    "    else:\n",
    "        # Not a census feature\n",
    "        return feature_name\n",
    "\n",
    "    partial_dir = feature_name_code.lower()\n",
    "    base_path = 'data/census/'\n",
    "    \n",
    "    matching_dir = glob.glob(os.path.join(base_path, partial_dir + '*'))\n",
    "    \n",
    "    if matching_dir:\n",
    "        target_dir = matching_dir[0]\n",
    "        \n",
    "        dataset_name = after_underscore = target_dir.split(\"_\", 1)[1] # characters following the code.\n",
    "        dataset_name = dataset_name.replace('_', ' ').title()\n",
    "        file_path = os.path.join(target_dir, f'{data_type}{year}.{feature_name_code}-Column-Metadata.csv')\n",
    "\n",
    "        df_columns = pd.read_csv(file_path)\n",
    "        label = df_columns[df_columns['Column Name'] == feature_name].values[0][1]\n",
    "        parts = label.split('!!')\n",
    "        short_label = ' | '.join(parts)\n",
    "        feature_label = f'{feature_name} | {dataset_name} | {short_label}'\n",
    "\n",
    "        return feature_label\n",
    "\n",
    "\n",
    "def plot_pca(X_pca, pca, top_features, merged_df, pc_x, pc_y, filename, year):\n",
    "    output_dir = 'output/personas'\n",
    "    \n",
    "    pc_x_label = f\"PC{pc_x} ({pca.explained_variance_ratio_[pc_x-1]:.1%})\\nTop: {', '.join(top_features[f'PC{pc_x}'])}\"\n",
    "    pc_y_label = f\"PC{pc_y} ({pca.explained_variance_ratio_[pc_y-1]:.1%})\\nTop: {', '.join(top_features[f'PC{pc_y}'])}\"\n",
    "\n",
    "    label_text = []\n",
    "    top_feature_list = ['X Axis'] + ['------------'] + top_features[f'PC{pc_x}'] + ['======'] + ['Y Axis'] + ['------------'] + top_features[f'PC{pc_y}']\n",
    "    for top_feature in top_feature_list:\n",
    "        if 'Axis' in top_feature or top_feature in ['------------', '======']:\n",
    "            label_text.append(top_feature)\n",
    "        else:\n",
    "            label_text.append(makeCensusFeatureLabels(top_feature, int(year)))\n",
    "    label_text_str = '\\n'.join(label_text)\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    scatter = plt.scatter(X_pca[:, pc_x-1], X_pca[:, pc_y-1], c=merged_df['cluster'], cmap='plasma', alpha=0.5)\n",
    "    unique_clusters = np.unique(merged_df['cluster'])\n",
    "    handles = [\n",
    "        plt.Line2D([], [], marker='o', color='w',\n",
    "                   markerfacecolor=plt.cm.plasma(c / max(unique_clusters)),\n",
    "                   label=f\"Cluster {c}\", markersize=10)\n",
    "        for c in unique_clusters\n",
    "    ]\n",
    "    plt.legend(handles=handles, title='Voter Personas')\n",
    "    plt.title(f'Voter Personas - PCA PC{pc_x} vs PC{pc_y}')\n",
    "    plt.xlabel(pc_x_label)\n",
    "    plt.ylabel(pc_y_label)\n",
    "    plt.gcf().text(0.5, 0.5, label_text_str, fontsize=6, va='center', ha='left',\n",
    "                   bbox=dict(facecolor='white', edgecolor='gray', boxstyle='round,pad=0.5'))\n",
    "    plt.tight_layout(rect=[0, 0, 0.5, 1])\n",
    "    plt.savefig(f'{output_dir}/{filename}')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def prepare_features_for_clustering(merged_df):\n",
    "    print(\"Preparing features for clustering...\")\n",
    "    exclude_cols = [\n",
    "        'standardized_id_num', 'standardized_id', \n",
    "        'geometry', 'geometry_tract', 'shapestarea', 'shapestlength', \n",
    "        'awater_tract', 'aland_tract', 'nearest_tract', 'tractce_tract', \n",
    "        'geoid_tract', 'geoidfq_tract', 'objectid', 'name_tract', \n",
    "        'nearest_bound_census_tract', 'nearest_bound_school_district', 'nearest_bound_zipcode',\n",
    "        'partisanship', 'partisanship_change', 'partisanship_change_amount',\n",
    "        'dem_share', 'rep_share', 'oth_share',\n",
    "        'dem_share_prev', 'rep_share_prev', 'oth_share_prev',\n",
    "        'dem_share_change', 'rep_share_change', 'oth_share_change',\n",
    "        'dem_votes', 'rep_votes', 'oth_votes',\n",
    "        'dem_votes_change', 'rep_votes_change', 'oth_votes_change',\n",
    "        'predicted_partisanship_change',\n",
    "        'total_votes', 'registered_voters', 'registered_voters_change', 'turnout_pct',\n",
    "        'City/Township Description', 'District Code', 'Election Type',\n",
    "        'Michigan County Code', 'Office Description', 'Precinct Label', 'Status Code',\n",
    "        'County Name', 'Precinct Number', 'Election Year',\n",
    "        'Office Code', 'Census County Code', 'City/Township Code', 'Ward Number',\n",
    "        'county', 'office', 'electionye', 'locale_full', 'subdivision_fips', 'ward_num', \n",
    "        'county_fips', 'precinct_num', 'precinct_wp_id',\n",
    "    ]\n",
    "    \n",
    "    object_cols = merged_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    exclude_cols.extend(object_cols)\n",
    "    \n",
    "    numeric_cols = merged_df.select_dtypes(include=np.number).columns.tolist()\n",
    "    valid_features = [col for col in numeric_cols if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"Selected {len(valid_features)} numeric features for clustering\")\n",
    "    return valid_features\n",
    "\n",
    "\n",
    "def visualize_personas_common(merged_df, valid_features, year):\n",
    "    if merged_df.empty or 'cluster' not in merged_df.columns or len(valid_features) < 2:\n",
    "        print(\"Not enough data for visualizations\")\n",
    "        return None, None, None\n",
    "\n",
    "    X = handle_missing_values(merged_df, valid_features)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    pca = PCA(n_components=3, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i+1}' for i in range(pca.n_components)], index=valid_features)\n",
    "    top_features = {f'PC{i+1}': loadings[f'PC{i+1}'].abs().sort_values(ascending=False).head(3).index.tolist() for i in range(3)}\n",
    "\n",
    "    return X_pca, pca, top_features, X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39827f07-8f41-4511-9c97-051ffb727d06",
   "metadata": {},
   "source": [
    "### Persona Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627bf48a-bbd3-45f0-aca3-f6229ec3bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_persona(cluster_id, name, cluster_size, cluster_size_percent, dem_share_fmt, rep_share_fmt, dem_change_fmt, rep_change_fmt, partisan_change_category, defining_features, center_values, standardized_id_nums):\n",
    "    persona = {\n",
    "        'cluster_id': cluster_id,\n",
    "        'name': name,\n",
    "        'size': f\"{cluster_size} precincts ({cluster_size_percent:.1f}%)\" if isinstance(cluster_size, (int, float)) else \"Unknown\",\n",
    "        'partisan_base': f\"D: {dem_share_fmt}, R: {rep_share_fmt}\",\n",
    "        'partisan_trend': f\"Change: D {dem_change_fmt}, R {rep_change_fmt}\",\n",
    "        'partisan_category': partisan_change_category,\n",
    "        'defining_features': defining_features,\n",
    "        'center_values': center_values,\n",
    "        'standardized_id_nums': standardized_id_nums,\n",
    "    }\n",
    "\n",
    "    return persona\n",
    "\n",
    "\n",
    "def create_persona_descriptions(merged_df, centers, cluster_analysis, valid_features):\n",
    "    print(\"Creating persona descriptions...\")\n",
    "\n",
    "    personas = []\n",
    "\n",
    "    for i in range(len(centers)):\n",
    "        standardized_id_nums = []\n",
    "        for standardized_id_num in cluster_analysis['standardized_id_nums']:\n",
    "            standardized_id_nums.append(standardized_id_num)\n",
    "        \n",
    "        cluster_id = i\n",
    "        cluster_size, cluster_size_percent = calculate_cluster_stats(cluster_id, cluster_analysis)\n",
    "        dem_share, rep_share, dem_change, rep_change, partisan_change_category, partisan_shift = extract_voting_patterns(cluster_id, cluster_analysis)\n",
    "        partisan_base, trend = determine_partisan_base_and_trend(dem_share, rep_share, partisan_shift)\n",
    "        defining_features = get_defining_features(centers, cluster_id)\n",
    "        dem_share_fmt, rep_share_fmt, dem_change_fmt, rep_change_fmt = format_shares_and_changes(dem_share, rep_share, dem_change, rep_change)\n",
    "\n",
    "        # Replace bad KMeans lat/lon with valid lat/lon\n",
    "        center_values = dict(zip(valid_features, centers.iloc[cluster_id]))\n",
    "        lat_lon = merged_df[merged_df['cluster'] == cluster_id][['latitude', 'longitude']].mean()\n",
    "        center_values['latitude'] = lat_lon['latitude']\n",
    "        center_values['longitude'] = lat_lon['longitude']\n",
    "\n",
    "        persona = create_persona(\n",
    "            cluster_id,\n",
    "            f\"Persona {cluster_id + 1}: {partisan_base} {trend}\",\n",
    "            cluster_size,\n",
    "            cluster_size_percent,\n",
    "            dem_share_fmt,\n",
    "            rep_share_fmt,\n",
    "            dem_change_fmt,\n",
    "            rep_change_fmt,\n",
    "            partisan_change_category,\n",
    "            defining_features,\n",
    "            center_values,\n",
    "            standardized_id_nums,\n",
    "        )\n",
    "\n",
    "        personas.append(persona)\n",
    "\n",
    "    return personas\n",
    "\n",
    "\n",
    "def create_partisan_summary(personas, year, office):\n",
    "    dem_shares = []\n",
    "    rep_shares = []\n",
    "    cluster_ids = []\n",
    "    persona_names = []\n",
    "    \n",
    "    for persona in personas:\n",
    "        dem_share = float(persona['partisan_base'].split('D: ')[1].split('%')[0])\n",
    "        rep_share = float(persona['partisan_base'].split('R: ')[1].split('%')[0])\n",
    "        \n",
    "        dem_shares.append(dem_share)\n",
    "        rep_shares.append(rep_share)\n",
    "        cluster_ids.append(persona['cluster_id'])\n",
    "        \n",
    "        name_parts = persona['name'].split(':')\n",
    "        if len(name_parts) > 1:\n",
    "            persona_names.append(name_parts[0])\n",
    "        else:\n",
    "            persona_names.append(f\"Persona {persona['cluster_id']}\")\n",
    "    \n",
    "    if cluster_ids:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        x = np.arange(len(cluster_ids))\n",
    "        width = 0.35\n",
    "        \n",
    "        sorted_indices = np.argsort(cluster_ids)\n",
    "        dem_shares_sorted = [dem_shares[i] for i in sorted_indices]\n",
    "        rep_shares_sorted = [rep_shares[i] for i in sorted_indices]\n",
    "        persona_names_sorted = [persona_names[i] for i in sorted_indices]\n",
    "        \n",
    "        plt.bar(x - width/2, dem_shares_sorted, width, label='Democrat %', color='blue', alpha=0.7)\n",
    "        plt.bar(x + width/2, rep_shares_sorted, width, label='Republican %', color='red', alpha=0.7)\n",
    "        \n",
    "        for i, v in enumerate(dem_shares_sorted):\n",
    "            plt.text(i - width/2, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9) \n",
    "        for i, v in enumerate(rep_shares_sorted):\n",
    "            plt.text(i + width/2, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "        \n",
    "        plt.xlabel('Voter Persona')\n",
    "        plt.ylabel('Vote Share (%)')\n",
    "        plt.title('Partisan Base by Voter Persona')\n",
    "        plt.xticks(x, persona_names_sorted, rotation=45, ha='right')\n",
    "        plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
    "        plt.axhline(y=50, color='black', linestyle='--', alpha=0.5)\n",
    "        plt.text(len(cluster_ids)-1, 50.5, '50%', ha='right', va='bottom', color='black')\n",
    "        plt.ylim(0, max(max(dem_shares), max(rep_shares)) * 1.15)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'output/personas/personas_summary_{year}_{office.replace(' ', '_').replace('.', '')}.png')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def create_voter_personas(year, office, n_clusters=10, use_prediction_features_only=True, geo_sensitive=True, geo_weight=4):\n",
    "    print(f\"Creating voter personas for {year} {office}...\")\n",
    "\n",
    "    office_file = office.replace(\".\", \"\").replace(\" \", \"_\")\n",
    "    predictions, precinct_data = load_precinct_data(year, office)\n",
    "    census_dataset_dfs = load_census_data(census_datasets, year, office)\n",
    "    merged_df = merge_datasets(precinct_data, census_dataset_dfs)\n",
    "\n",
    "    feature_importance_file = f'data/generated_data/feature_rankings_{year}_{office_file}.csv'\n",
    "    feature_rankings = pd.read_csv(feature_importance_file)\n",
    "\n",
    "    exclude_values = [\n",
    "        'dem_share_change', 'rep_share_change', 'oth_share_change',\n",
    "        'dem_share_prev', 'rep_share_prev', 'oth_share_prev',\n",
    "        'turnout_pct_change', 'partisanship_change_amount'\n",
    "    ]\n",
    "    feature_rankings = feature_rankings[~feature_rankings['Feature name'].isin(exclude_values)]\n",
    "\n",
    "    top_n = 31\n",
    "    importance_col = 'RandomForest'\n",
    "    top_features = feature_rankings.sort_values(importance_col, ascending=False).head(top_n)['Feature name'].tolist()\n",
    "    valid_features = [feat for feat in top_features if feat in merged_df.columns]\n",
    "\n",
    "    print(f\"Selected {len(valid_features)} features for clustering\")\n",
    "\n",
    "    merged_df, centers = create_clusters(merged_df, valid_features, n_clusters=n_clusters, geo_sensitive=True, geo_weight=4)\n",
    "    cluster_analysis = analyze_voting_patterns(merged_df)\n",
    "    voter_regions = create_voter_regions(cluster_analysis, office)\n",
    "    personas = create_persona_descriptions(merged_df, centers, cluster_analysis, valid_features)\n",
    "    \n",
    "    create_partisan_summary(personas, year, office)\n",
    "    \n",
    "    visualize_personas(merged_df, valid_features, personas, year, office)\n",
    "    report = generate_personas_report(personas, valid_features, merged_df, year, office)\n",
    "\n",
    "    print(f\"Created {n_clusters} voter personas. Reports and visualizations saved to output/personas/\")\n",
    "\n",
    "    return personas, merged_df, voter_regions\n",
    "\n",
    "\n",
    "def create_voter_regions(cluster_analysis, office):\n",
    "    dfs = {}\n",
    "    \n",
    "    for cluster_id in cluster_analysis['cluster']:\n",
    "        standardized_id_nums = cluster_analysis['standardized_id_nums'][cluster_id]\n",
    "        df_standardized_id_nums = pd.DataFrame(standardized_id_nums, columns=['standardized_id_num'])\n",
    "        df_standardized_id_nums['standardized_id_num'] = df_standardized_id_nums['standardized_id_num'].apply(lambda x: str(x).zfill(13))\n",
    "        \n",
    "        df_precincts = gpd.read_file(f'data/generated_data/df_05_precinct_mapped_merged_{YEAR}_{office.replace('.', '').replace(' ', '_')}.geojson')\n",
    "        df_precincts = pd.merge(df_standardized_id_nums, df_precincts, on=\"standardized_id_num\", how=\"left\")\n",
    "        df_precincts = df_precincts[['standardized_id_num', 'geometry']]\n",
    "        df_precincts = gpd.GeoDataFrame(df_precincts, geometry='geometry')\n",
    "\n",
    "        dfs[cluster_id] = df_precincts\n",
    "\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def generate_personas_report(personas, valid_features, merged_df, year, office):\n",
    "    print(\"Generating comprehensive personas report...\")\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"# Voter Persona Analysis\\n\")\n",
    "    report.append(\"## Overview\\n\")\n",
    "    report.append(\"This analysis identifies distinct voter groups based on demographic characteristics and voting patterns \")\n",
    "    report.append(\"from precinct-level electoral data and census demographics. \")\n",
    "    report.append(f\"Using {len(valid_features)} key demographic features, we identified {len(personas)} distinct voter personas.\\n\")\n",
    "    \n",
    "    report.append(\"## Features Used for Analysis\\n\")\n",
    "    report.append(\"The following demographic features were most predictive of voting pattern changes:\\n\")\n",
    "    for i, feat in enumerate(valid_features, 1):\n",
    "        feature_label = makeCensusFeatureLabels(feat, int(year))\n",
    "        report.append(f\"{i}. `{feature_label}`\\n\")\n",
    "\n",
    "    report.append(\"\\n\")\n",
    "    report.append(\"## Methodology\\n\")\n",
    "    report.append(\"Voter personas were created using K-means clustering on standardized demographic features. \")\n",
    "    report.append(\"Each persona represents a group of precincts with similar characteristics \")\n",
    "    report.append(\"and voting patterns.\\n\")\n",
    "    \n",
    "    report.append(\"\\n\")\n",
    "    report.append(\"## Voter Persona Summary\\n\")\n",
    "    report.append(\"| Voter Persona | Size | Partisan Base | Trend | Change Category |\\n\")\n",
    "    report.append(\"|---------|------|---------------|-------|----------------|\\n\")\n",
    "    \n",
    "    for p in personas:\n",
    "        report.append(f\"| {p['name']} | {p['size']} | {p['partisan_base']} | {p['partisan_trend']} | {p['partisan_category']} |\\n\")\n",
    "    report.append(\"\\n\")\n",
    "    \n",
    "    report.append(\"## Detailed Voter Persona Profiles\\n\")\n",
    "    for p in personas:\n",
    "        if 'center_values' in p and p['center_values']:\n",
    "            center_values = p['center_values']\n",
    "            latitude = center_values.pop('latitude')\n",
    "            longitude = center_values.pop('longitude')\n",
    "\n",
    "        report.append(f\"### {p['name']}\\n\")\n",
    "        report.append(f\"**Size**: {p['size']}\\n\")\n",
    "        report.append(f\"<br>**Partisan Base**: {p['partisan_base']}\\n\")\n",
    "        report.append(f\"<br>**Partisan Trend**: {p['partisan_trend']}\\n\")\n",
    "        report.append(f\"<br>**Change Category**: {p['partisan_category']}\\n\")\n",
    "        report.append(f\"<br>**Lat/Lon**: {latitude:.6f}, {longitude:.6f} | [View Map](https://www.google.com/maps/place/{latitude},{longitude}){{:target='_blank'}}\\n\")\n",
    "        \n",
    "        report.append(\"#### Key Demographics\\n\")\n",
    "        \n",
    "        if 'center_values' in p and p['center_values']:\n",
    "            center_values = p['center_values']\n",
    "\n",
    "            feature_medians = {\n",
    "                feat: pd.to_numeric(merged_df[feat], errors='coerce').median()\n",
    "                for feat in valid_features\n",
    "                if feat in merged_df.columns\n",
    "            }\n",
    "            \n",
    "            feature_diffs = {\n",
    "                feat: (center_values[feat] - feature_medians[feat]) / feature_medians[feat] * 100\n",
    "                for feat in valid_features if feat in center_values and feature_medians[feat] != 0\n",
    "            }\n",
    "            \n",
    "            sorted_features = sorted(feature_diffs.keys(), key=lambda x: abs(feature_diffs[x]), reverse=True)\n",
    "            \n",
    "            for feat in sorted_features[:31]:\n",
    "                value = center_values[feat]\n",
    "                diff = feature_diffs[feat]\n",
    "                direction = \"higher\" if diff > 0 else \"lower\"\n",
    "                feature_label = makeCensusFeatureLabels(feat, int(year))\n",
    "                report.append(f\"- **{feature_label}**:<br>{value:.2f} ({abs(diff):.1f}% {direction} than median)\\n\")\n",
    "            report.append(\"\\n\")\n",
    "\n",
    "\n",
    "        if LLM_ACTIVE == True:\n",
    "            print('Generated LLM persona summary...')\n",
    "            \n",
    "            # ChatGPT interpretation of demo features\n",
    "            prompt_intro = '''You are a political demographer and researcher. You write voter personas much like a dossier. \n",
    "                             Voter personas are created by carefully synthesizing census data. Take this numbered list of data points. \n",
    "                             Note each category and its value, as well as deviation from the median. Make sure to write at \n",
    "                             least three paragraphs, with headings, that characterize this voter persona. Also, make sure \n",
    "                             to write a summarized bulleted list. Use the tone and writing style of Robert McNamara, the former \n",
    "                             Secretary of Defense for John F Kennedy and Lyndon Johnson. Be sharp, concise, and objective.\n",
    "                             Use adjectives only when justified by the data. Your audience are top political decision makers.\\n'''\n",
    "            prompt_intro += '''Formatting: use only h4 headings above paragraphs. Do not number lists. Above the bulleted list, \n",
    "                               add an h4 heading that says \"Key Insights\".'''\n",
    "            prompt = prompt_intro + ' '.join(llm_feature_labels)\n",
    "            llm_summary = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            response = llm_summary.choices[0].message.content\n",
    "            report.append(\"### LLM-Generated Voter Profile\\n\")\n",
    "            report.append(response)\n",
    "        \n",
    "        report.append(\"\\n---\\n\\n\")\n",
    "    \n",
    "    report_path = os.path.join('output/personas', f'personas_report_{year}_{office.replace(' ', '_').replace('.', '')}.md')\n",
    "    with open(report_path, 'w') as file:\n",
    "        file.write(''.join(report))\n",
    "    \n",
    "    print(f\"Comprehensive report saved to {report_path}\")\n",
    "    return ''.join(report)\n",
    "\n",
    "\n",
    "def visualize_personas(merged_df, valid_features, personas, year, office):\n",
    "    print(\"Visualizing personas...\")\n",
    "\n",
    "    X_pca, pca, top_features, X = visualize_personas_common(merged_df, valid_features, year)\n",
    "    if X_pca is None:\n",
    "        return\n",
    "    \n",
    "    plot_pca(X_pca, pca, top_features, merged_df, 1, 2, f'personas_pca_pc1_vs_pc2_{year}_{office.replace(' ', '_').replace('.', '')}.png', year)\n",
    "    plot_pca(X_pca, pca, top_features, merged_df, 2, 3, f'personas_pca_pc1_vs_pc2_{year}_{office.replace(' ', '_').replace('.', '')}.png', year)\n",
    "    \n",
    "    if not personas or not all('center_values' in p for p in personas):\n",
    "        print(\"No valid persona data for visualization\")\n",
    "        return\n",
    "    \n",
    "    all_feature_values = {feat: [p['center_values'][feat] for p in personas if feat in p['center_values']] for feat in valid_features}\n",
    "    feature_variances = {feat: np.std(values) for feat, values in all_feature_values.items() if values}\n",
    "    radar_features = sorted(feature_variances, key=feature_variances.get, reverse=True)[:min(31, len(feature_variances))]\n",
    "    radar_features.remove('latitude')\n",
    "    radar_features.remove('longitude')\n",
    "\n",
    "    featuresUsed(radar_features)\n",
    "\n",
    "    visualize_personas_radar(personas, valid_features, radar_features, year, office, 'individual', X)\n",
    "\n",
    "\n",
    "def visualize_personas_radar(personas, valid_features, radar_features, year, office, suffix, X):\n",
    "    feature_min = {feat: X[feat].min() for feat in radar_features}\n",
    "    feature_max = {feat: X[feat].max() for feat in radar_features}\n",
    "\n",
    "    for persona in personas:\n",
    "        center_values = persona.get('center_values', {})\n",
    "        if not center_values:\n",
    "            continue\n",
    "        \n",
    "        fig = plt.figure(figsize=(30, 30))\n",
    "        ax = fig.add_subplot(111, polar=True)\n",
    "        N = len(radar_features)\n",
    "        angles = [n / float(N) * 2 * np.pi for n in range(N)] + [0]\n",
    "\n",
    "        values_norm = [(center_values.get(feat, 0) - feature_min[feat]) / (feature_max[feat] - feature_min[feat]) if feature_max[feat] > feature_min[feat] else 0.5 for feat in radar_features]\n",
    "        values_norm.append(values_norm[0])\n",
    "        \n",
    "        radar_features_named = [makeCensusFeatureLabels(f, int(year)) for f in radar_features]\n",
    "        title_text = f'{persona['name']}'\n",
    "        \n",
    "        ax.plot(angles, values_norm, linewidth=2, linestyle='solid')\n",
    "        ax.fill(angles, values_norm, alpha=0.25)\n",
    "        ax.set_rlim(0, 1)\n",
    "        plt.xticks(angles[:-1], radar_features_named, size=6)\n",
    "        plt.title(title_text, size=15, y=1.1)\n",
    "        plt.figtext(0.5, 0.01,\n",
    "                    f\"Size: {persona['size']}\\nBase: {persona['partisan_base']}\\nTrend: {persona['partisan_trend']}\\nCategory: {persona['partisan_category']}\",\n",
    "                    ha=\"center\", fontsize=12, bbox={\"facecolor\":\"orange\", \"alpha\":0.2, \"pad\":5})\n",
    "        plt.tight_layout()\n",
    "        safe_name = persona['name'].lower().replace(\" \", \"_\").replace(\":\", \"\").replace(\"-\", \"-\")\n",
    "        plt.savefig(f'output/personas/persona_{safe_name}_{suffix}_{year}_{office.replace(' ', '_').replace('.', '')}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b48e29-6e2f-4bb8-80d6-c2c77812bbdf",
   "metadata": {},
   "source": [
    "### Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0dd701-b975-463d-b5c2-f341c79c03e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "personas, merged_df, voter_regions = create_voter_personas(year=YEAR, office='US House', n_clusters=30, geo_sensitive=True, geo_weight=4)\n",
    "\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5ccbc-a5f2-4d48-9ce6-2c0d12a4ffa3",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b79cc4-d894-4771-97ef-541fdcd672c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_color(c1, c2, t):\n",
    "    \"\"\"\n",
    "    Linearly interpolate between two RGB colors.\n",
    "    \"\"\"\n",
    "    return tuple(int(c1[i] + (c2[i] - c1[i]) * t) for i in range(3))\n",
    "\n",
    "\n",
    "def get_color_from_share(dem_share):\n",
    "    \"\"\"\n",
    "    Maps a dem_share (0-100) to a color on a red → orange → yellow → green → blue gradient.\n",
    "    \"\"\"\n",
    "    t = dem_share / 100  # normalize to 0–1\n",
    "\n",
    "    stops = [\n",
    "        (0.00, (255, 0, 0)),     # Red\n",
    "        (0.25, (255, 165, 0)),   # Orange\n",
    "        (0.50, (255, 255, 0)),   # Yellow\n",
    "        (0.75, (0, 255, 0)),     # Green\n",
    "        (1.00, (0, 0, 255)),     # Blue\n",
    "    ]\n",
    "    \n",
    "    # Find two stops for interpolation\n",
    "    for i in range(len(stops) - 1):\n",
    "        left_t, left_color = stops[i]\n",
    "        right_t, right_color = stops[i + 1]\n",
    "        \n",
    "        if left_t <= t <= right_t:\n",
    "            # Normalize btwn two stops\n",
    "            local_t = (t - left_t) / (right_t - left_t)\n",
    "            return interpolate_color(left_color, right_color, local_t)\n",
    "\n",
    "\n",
    "def get_swing_color(dem_shift, intensity=10.0):\n",
    "    \"\"\"\n",
    "    Maps dem_shift (-100 to 100) to RGB from red to blue using a tanh curve.\n",
    "    - intensity: how sharply the color ramps up from center (higher = more contrast in small changes)\n",
    "    \"\"\"\n",
    "    x = dem_shift / 100.0  # Normalize to [-1, 1]\n",
    "    x_scaled = math.tanh(intensity * x)  # Nonlinear stretching within [-1, 1]\n",
    "    t = (x_scaled + 1) / 2 # Rescale to [0, 1] for color mapping\n",
    "    red = int(255 * (1 - t))\n",
    "    blue = int(255 * t)\n",
    "    green = 0\n",
    "    return (red, green, blue)\n",
    "\n",
    "\n",
    "def rgb_to_hex(rgb):\n",
    "    return '#{:02x}{:02x}{:02x}'.format(*rgb)\n",
    "\n",
    "\n",
    "def get_distinct_color_palette(n=40):\n",
    "    base_colors = [\n",
    "        \"#e6194b\", \"#3cb44b\", \"#ffe119\", \"#0082c8\", \"#f58231\",\n",
    "        \"#911eb4\", \"#46f0f0\", \"#f032e6\", \"#d2f53c\", \"#fabebe\",\n",
    "        \"#008080\", \"#e6beff\", \"#aa6e28\", \"#fffac8\", \"#800000\",\n",
    "        \"#aaffc3\", \"#808000\", \"#ffd8b1\", \"#000080\", \"#808080\",\n",
    "        \"#a6cee3\", \"#1f78b4\", \"#b2df8a\", \"#33a02c\", \"#fb9a99\",\n",
    "        \"#e31a1c\", \"#fdbf6f\", \"#ff7f00\", \"#cab2d6\", \"#6a3d9a\",\n",
    "        \"#ffff99\", \"#b15928\", \"#bc80bd\", \"#ccebc5\", \"#ffed6f\",\n",
    "        \"#bcbd22\", \"#17becf\", \"#8dd3c7\", \"#bebada\", \"#fb8072\"\n",
    "    ]\n",
    "    if n > len(base_colors):\n",
    "        raise ValueError(f\"Requesting {n} colors, but only {len(base_colors)} are available.\")\n",
    "    return base_colors[:n]\n",
    "\n",
    "\n",
    "def polygon_to_patches(geom, color, hatch):\n",
    "    from matplotlib.patches import PathPatch\n",
    "    from matplotlib.path import Path as MplPath\n",
    "    import numpy as np\n",
    "\n",
    "    def create_path_from_coords(coords):\n",
    "        verts = np.array(coords)\n",
    "        codes = [MplPath.MOVETO] + [MplPath.LINETO] * (len(verts) - 2) + [MplPath.CLOSEPOLY]\n",
    "        return MplPath(verts, codes)\n",
    "\n",
    "    patches = []\n",
    "\n",
    "    if geom.geom_type == 'Polygon':\n",
    "        path = create_path_from_coords(geom.exterior.coords)\n",
    "        patch = PathPatch(path, facecolor=color, edgecolor='black', linewidth=0.01, hatch=hatch)\n",
    "        patches.append(patch)\n",
    "\n",
    "        for interior in geom.interiors:\n",
    "            hole_path = create_path_from_coords(interior.coords)\n",
    "            hole_patch = PathPatch(hole_path, facecolor='white', edgecolor='black', linewidth=0.01)\n",
    "            patches.append(hole_patch)\n",
    "\n",
    "    elif geom.geom_type == 'MultiPolygon':\n",
    "        for part in geom.geoms:\n",
    "            patches.extend(polygon_to_patches(part, color, hatch))\n",
    "\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b184e6-1cce-4a9e-afd3-e6db486476bd",
   "metadata": {},
   "source": [
    "### Voter regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b214ff-1e03-4dfe-93fb-3087f95b977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotVoterRegion(personas, year, office, subregion=None, labels=True):\n",
    "    fig, ax = plt.subplots(figsize=(80, 80))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    \n",
    "    # Cluster colors\n",
    "    colors = get_distinct_color_palette(len(personas))\n",
    "\n",
    "    # Hatch patterns for similar colors\n",
    "    hatch_patterns = ['//', '\\\\\\\\', 'xx', '++', '--', '..', '**', 'oo']\n",
    "    rgb_colors = [to_rgb(c) for c in colors]\n",
    "    dist_matrix = squareform(pdist(rgb_colors))\n",
    "    similarity_threshold = 0.25\n",
    "    cluster_hatches = [None] * len(colors)\n",
    "    hatch_index = 0\n",
    "    for i in range(len(colors)):\n",
    "        for j in range(i + 1, len(colors)):\n",
    "            if dist_matrix[i][j] < similarity_threshold:\n",
    "                if cluster_hatches[i] is None:\n",
    "                    cluster_hatches[i] = hatch_patterns[hatch_index % len(hatch_patterns)]\n",
    "                    hatch_index += 1\n",
    "                if cluster_hatches[j] is None:\n",
    "                    cluster_hatches[j] = hatch_patterns[hatch_index % len(hatch_patterns)]\n",
    "                    hatch_index += 1\n",
    "    \n",
    "    # SE Michigan bounds\n",
    "    se_mi_lon_min, se_mi_lon_max = -84.5, -82.5\n",
    "    se_mi_lat_min, se_mi_lat_max = 41.8, 43.5\n",
    "    \n",
    "    metro_label_drawn = False\n",
    "\n",
    "    if subregion == 'Southeast Michigan':\n",
    "        lon_min, lon_max = se_mi_lon_min, se_mi_lon_max\n",
    "        lat_min, lat_max = se_mi_lat_min, se_mi_lat_max\n",
    "        ax.set_xlim(lon_min, lon_max)\n",
    "        ax.set_ylim(lat_min, lat_max)\n",
    "        se_mi_bbox = box(lon_min, lat_min, lon_max, lat_max)\n",
    "    else:\n",
    "        ax.autoscale()\n",
    "        se_mi_bbox = box(se_mi_lon_min, se_mi_lat_min, se_mi_lon_max, se_mi_lat_max)\n",
    "    \n",
    "    legend_patches = []\n",
    "    cluster_id = 0\n",
    "    \n",
    "    for persona in personas:\n",
    "        name = persona['name']\n",
    "        name = name.replace('Persona', 'Region') # treat as regions\n",
    "        \n",
    "        color = colors[cluster_id]\n",
    "        patch = mpatches.Patch(facecolor=color, edgecolor='black', label=name, hatch=cluster_hatches[cluster_id])\n",
    "        legend_patches.append(patch)\n",
    "        \n",
    "        voter_region = voter_regions[cluster_id]\n",
    "        voter_region.boundary.plot(ax=ax, color=\"black\", linewidth=0.1)\n",
    "\n",
    "        # Plot colors/hatching\n",
    "        for geom in voter_region.geometry:\n",
    "            for patch in polygon_to_patches(geom, color=color, hatch=cluster_hatches[cluster_id]):\n",
    "                ax.add_patch(patch)\n",
    "        \n",
    "        # Combine geometries within cluster\n",
    "        dissolved = voter_region.dissolve()\n",
    "        disconnected_parts = dissolved.explode(index_parts=False)\n",
    "\n",
    "        # Labeling logic\n",
    "        if labels == True:\n",
    "            for geom in disconnected_parts.geometry:\n",
    "                part_gs = gpd.GeoSeries([geom], crs='EPSG:4326').to_crs(epsg=3857)\n",
    "            \n",
    "                # Min distance between contiguous regions for labeling\n",
    "                area_km2 = part_gs.area.iloc[0] / 1e6\n",
    "                if subregion != None:\n",
    "                    if area_km2 < 225:\n",
    "                        continue\n",
    "                else:\n",
    "                    if area_km2 < 25:\n",
    "                        continue\n",
    "            \n",
    "                # Centroid for label (reprojected back to WGS84)\n",
    "                centroid = part_gs.centroid.to_crs(epsg=4326).iloc[0]\n",
    "    \n",
    "                # Don't plot anything outside subregion bounding box\n",
    "                if subregion != None:\n",
    "                    if not (lon_min <= centroid.x <= lon_max and lat_min <= centroid.y <= lat_max):\n",
    "                        continue\n",
    "    \n",
    "                # Suppress local labels inside Metro Detroit when plotting full state\n",
    "                if not subregion and se_mi_bbox.contains(centroid):\n",
    "                    if not metro_label_drawn:\n",
    "                        ax.text(\n",
    "                            -83.3, 42.5,\n",
    "                            \"Metro Detroit\",\n",
    "                            fontsize=48,\n",
    "                            ha='center',\n",
    "                            va='center',\n",
    "                            color='black',\n",
    "                            weight='bold',\n",
    "                            bbox=dict(facecolor='white', alpha=0.8, edgecolor='none', boxstyle='round,pad=0.3')\n",
    "                        )\n",
    "                        metro_label_drawn = True\n",
    "                    continue\n",
    "                \n",
    "                # Symbol position offset\n",
    "                x_offset = 0.005\n",
    "                symbol_x = centroid.x - x_offset\n",
    "                text_x = centroid.x + x_offset\n",
    "                y = centroid.y\n",
    "    \n",
    "                # Subregions hve larger text\n",
    "                if subregion:\n",
    "                    fontsize = 21\n",
    "                else:\n",
    "                    fontsize = 14\n",
    "                \n",
    "                # Colored symbols\n",
    "                ax.text(\n",
    "                    symbol_x,\n",
    "                    y,\n",
    "                    '■',\n",
    "                    fontsize=fontsize,\n",
    "                    ha='right',\n",
    "                    va='center',\n",
    "                    color=color,\n",
    "                    weight='bold',\n",
    "                    bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.2')\n",
    "                )\n",
    "            \n",
    "                # Black text labels\n",
    "                ax.text(\n",
    "                    text_x,\n",
    "                    y,\n",
    "                    name,\n",
    "                    fontsize=fontsize,\n",
    "                    ha='left',\n",
    "                    va='center',\n",
    "                    color='black',\n",
    "                    weight='bold',\n",
    "                    bbox=dict(facecolor='white', alpha=0.7, edgecolor='none', boxstyle='round,pad=0.2')\n",
    "                )\n",
    "    \n",
    "        cluster_id += 1\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(\n",
    "        handles=legend_patches,\n",
    "        loc='upper left',\n",
    "        bbox_to_anchor=(1.02, 1),\n",
    "        fontsize=32,\n",
    "        title='Voter Regions',\n",
    "        title_fontsize=36,\n",
    "        handler_map={mpatches.Patch: HandlerPatch()},\n",
    "    )\n",
    "        \n",
    "    ax.margins(0)\n",
    "    ax.set_title('Voter Regions', fontsize=64)\n",
    "    ax.set_axis_off()\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "\n",
    "    # Append region info\n",
    "    filename_subregion = \"Statewide\" if subregion is None else subregion.replace(' ', '_').replace('.', '')\n",
    "    \n",
    "    plt.savefig('output/maps/regions/Voter_Region_' + str(year) + \"_\" + office.replace('.', '').replace(' ', '_') + \"_Map_\" + filename_subregion + \".png\", bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b787dc5-71cc-4695-aa56-bb13f724b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotVoterRegion(personas, YEAR, office=\"President\", subregion=None, labels=False)\n",
    "plotVoterRegion(personas, YEAR, office=\"President\", subregion='Southeast Michigan', labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d2eb4d-27f5-4edb-b6d6-7c5261d2f857",
   "metadata": {},
   "source": [
    "### Voter Region Leanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566d9577-eebd-4080-986c-c3fa46a41ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotVoterRegionLeanings(personas, voter_regions, year, office):\n",
    "    fig, ax = plt.subplots(figsize=(80, 80))\n",
    "    divider = make_axes_locatable(ax)\n",
    "    \n",
    "    # Define gradient (normalized to 0–1 range)\n",
    "    cmap = LinearSegmentedColormap.from_list(\n",
    "        'dem_share_cmap',\n",
    "        [\n",
    "            (0.00, (1.0, 0.0, 0.0)),     # Red\n",
    "            (0.25, (1.0, 0.65, 0.0)),    # Orange\n",
    "            (0.50, (1.0, 1.0, 0.0)),     # Yellow\n",
    "            (0.75, (0.0, 1.0, 0.0)),     # Green\n",
    "            (1.00, (0.0, 0.0, 1.0))      # Blue\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    cluster_id = 0\n",
    "    for persona in personas:\n",
    "        dem_share = float(re.search(r'(\\d+(?:\\.\\d+)?)%', persona['partisan_base'])[0][:-1])\n",
    "        # rep_share = re.findall(r'(\\d+(?:\\.\\d+)?)%', persona['partisan_base'])[1]\n",
    "        \n",
    "        color = rgb_to_hex(get_color_from_share(dem_share))\n",
    "        \n",
    "        voter_region = voter_regions[cluster_id]\n",
    "        voter_region.boundary.plot(ax=ax, color=\"black\", linewidth=0.1)\n",
    "        voter_region.plot(ax=ax, color=color, edgecolor=\"black\", linewidth=0.01)\n",
    "    \n",
    "        cluster_id += 1\n",
    "    \n",
    "    ax.margins(0)\n",
    "    ax.set_title('Voter Regions', fontsize=64)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    cax = divider.append_axes(\"right\", size=\"2%\", pad=0.5)\n",
    "    \n",
    "    # Colorbar\n",
    "    import matplotlib as mpl\n",
    "    norm = mpl.colors.Normalize(vmin=0, vmax=100)\n",
    "    cb = mpl.colorbar.ColorbarBase(cax, cmap=cmap, norm=norm, orientation='vertical')\n",
    "    cb.set_label('Democratic Share (%)', fontsize=32)\n",
    "    cb.ax.tick_params(labelsize=24)\n",
    "    \n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    \n",
    "    plt.savefig('output/maps/regions/Voter_Region_' + str(year) + \"_\" + office.replace('.', '').replace(' ', '_') + \"_Leaning_Map.png\", bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9104ac-d57b-4267-a29c-a19b52f1f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotVoterRegionLeanings(personas, voter_regions, YEAR, 'President')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ebe0a4-9d35-48d1-96d5-a01c4b71ba85",
   "metadata": {},
   "source": [
    "### Voter Region Shifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cade2be-021a-4789-bf0c-8e966d9619f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotVoterRegionShifts(personas, voter_regions, year, office):\n",
    "    fig, ax = plt.subplots(figsize=(80, 80))\n",
    "    divider = make_axes_locatable(ax)\n",
    "\n",
    "    # Define gradient (normalized to 0–1 range)\n",
    "    cmap = LinearSegmentedColormap.from_list(\n",
    "        'swing_cmap',\n",
    "        [\n",
    "            (0.00, (1.0, 0.0, 0.0)),  # Red\n",
    "            (0.50, (0.5, 0.0, 0.5)),  # Purple midpoint\n",
    "            (1.00, (0.0, 0.0, 1.0))   # Blue\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    cluster_id = 0\n",
    "    for persona in personas:\n",
    "        dem_share = float(re.findall(r'(-?\\d+(?:\\.\\d+)?)%', persona['partisan_trend'])[0])\n",
    "        rep_share = float(re.findall(r'(-?\\d+(?:\\.\\d+)?)%', persona['partisan_trend'])[1])\n",
    "        color = rgb_to_hex(get_swing_color(dem_share))\n",
    "        voter_region = voter_regions[cluster_id]\n",
    "        voter_region.boundary.plot(ax=ax, color=\"black\", linewidth=0.1)\n",
    "        voter_region.plot(ax=ax, color=color, edgecolor=\"black\", linewidth=0.01)\n",
    "        cluster_id += 1\n",
    "    \n",
    "    ax.margins(0)\n",
    "    ax.set_title('Voter Regions', fontsize=64)\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "    cax = divider.append_axes(\"right\", size=\"2%\", pad=0.5)\n",
    "    norm = mpl.colors.Normalize(vmin=-100, vmax=100)\n",
    "    cb = mpl.colorbar.ColorbarBase(cax, cmap=cmap, norm=norm, orientation='vertical')\n",
    "    cb.set_label('Partisan Shift (% → Democratic)', fontsize=32)\n",
    "    cb.ax.tick_params(labelsize=24)\n",
    "    \n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    \n",
    "    plt.savefig('output/maps/regions/Voter_Region_' + str(year) + \"_\" + office.replace('.', '').replace(' ', '_') + \"_Shift_Map.png\", bbox_inches='tight')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11505b4-bf96-467d-88fd-7b0498860da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotVoterRegionShifts(personas, voter_regions, YEAR, 'President')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd5eaf5-56ff-4f85-9aaf-c33c4a1b1285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
