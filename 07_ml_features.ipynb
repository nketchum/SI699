{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78014ab-4d05-46d9-a3f2-53c9323bc00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# President: 2016 (Trump), 2020 (Biden), 2024 (Trump)\n",
    "# Governor: 2018 (Whitmer), 2022 (Whitmer)\n",
    "# Secretary of State: 2018 (Benson), 2022 (Benson)\n",
    "# Attorney General: 2018 (Nessel), 2022 (Nessel)\n",
    "# U.S. Senate: 2014 (Peters), 2018 (Stabenow), 2020 (Peters), 2024 (Slotkin)\n",
    "# U.S. House: every cycle\n",
    "# State Senate: 2014, 2018, 2022\n",
    "# State House: every cycle\n",
    "\n",
    "# OFFICES = ['U.S. House', 'State House']\n",
    "# YEARS = ['2022', '2024']\n",
    "\n",
    "# OFFICES = ['U.S. Senate']\n",
    "# YEARS = ['2020', '2024']\n",
    "\n",
    "# OFFICES = ['State Senate']\n",
    "# YEARS = ['2022']\n",
    "\n",
    "# OFFICES = ['President']\n",
    "# YEARS = ['2024'] # you can only do 2024, given only two previous elections\n",
    "\n",
    "# Not enough data\n",
    "# # OFFICES = ['Governor', 'Secretary of State', 'Attorney General']\n",
    "# # YEARS = ['2018', '2022']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ce479-06d3-4ecf-ac51-c3c635bf926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff7d0a-9257-4f24-8f08-f8be5020a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LassoCV, LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import r2_score, accuracy_score, f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "import csv\n",
    "import gc\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44aa4f5-657e-4c8f-849c-bf8c5d2fe734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c5dfa-d0d4-4c31-8d1c-9a6c133ff329",
   "metadata": {},
   "source": [
    "### Compute partisanship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81c801-cf71-4cf8-a7e9-6a650e61d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_partisanship(row):\n",
    "    if row[\"dem_share_prev\"] >= 0.667:\n",
    "        return \"strong democrat\"\n",
    "    elif row[\"dem_share_prev\"] >= 0.501:\n",
    "        return \"leans democrat\"\n",
    "    elif row[\"rep_share_prev\"] >= 0.667:\n",
    "        return \"strong republican\"\n",
    "    elif row[\"rep_share_prev\"] >= 0.501:\n",
    "        return \"leans republican\"\n",
    "    elif row[\"oth_share_prev\"] >= 0.667:\n",
    "        return \"strong independent\"\n",
    "    elif row[\"oth_share_prev\"] >= 0.501:\n",
    "        return \"leans independent\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df150bd2-ba5b-4f0d-8f60-095ee963577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_partisan_change(row):\n",
    "    # Model with a number line, ignore \"other\" parties\n",
    "    # negative = left = more dem, positive = right = more rep\n",
    "    change = row[\"rep_share_change\"] - row[\"dem_share_change\"]\n",
    "    \n",
    "    if np.abs(change) >= 0.01:\n",
    "        if change > 0.5:\n",
    "            return \"gargantuanly more republican\"\n",
    "        if change > 0.35:\n",
    "            return \"massively more republican\"\n",
    "        if change > 0.25:\n",
    "            return \"much much more republican\"\n",
    "        if change > 0.15:\n",
    "            return \"much more republican\"\n",
    "        if change > 0.1:\n",
    "            return \"more republican\"\n",
    "        if change > 0.05:\n",
    "            return \"slightly more republican\"\n",
    "        elif change > 0.01:\n",
    "            return \"very slightly more republican\"\n",
    "        elif change > 0.005:\n",
    "            return \"infinitesimally more republican\"\n",
    "        elif change < -0.5:\n",
    "            return \"gargantuanly more democrat\"\n",
    "        elif change < -0.35:\n",
    "            return \"massively more democrat\"\n",
    "        elif change < -0.25:\n",
    "            return \"much much democrat\"\n",
    "        elif change < -0.15:\n",
    "            return \"much more democrat\"\n",
    "        elif change < -0.1:\n",
    "            return \"more democrat\"\n",
    "        elif change < -0.05:\n",
    "            return \"slightly more democrat\"\n",
    "        elif change < -0.01:\n",
    "            return \"very slightly more democrat\"\n",
    "        elif change < -0.005:\n",
    "            return \"infinitesimally more democrat\"\n",
    "    else:\n",
    "        return \"no change\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b7ee26-9482-4d1c-92cc-f751f5c33cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_partisan_change_amount(row):\n",
    "    # Model with a number line, ignore \"other\" parties\n",
    "    # negative = left = more dem, positive = right = more rep\n",
    "    change = row[\"rep_share_change\"] - row[\"dem_share_change\"]\n",
    "    return change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b70752-d333-4a7b-ab44-dbf0b741fc0d",
   "metadata": {},
   "source": [
    "### Predictions\n",
    "US House and State House for 2018, 2020, 2022 ~ 45 mins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7265e85-aa43-4b2b-8255-56c15b1ba86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "census_datasets = [\n",
    "    'b02001_race', 'b04007_ancestry', 'b05012_nativity_us', 'b08303_travel_time_work', \n",
    "    'b25003_housing_rentership', 'dp04_housing_characteristics', 'dp05_age_race', 's0101_age_sex', \n",
    "    's1101_households_families', 's1201_marital_status', 's1501_educational_attainment', 's1701_income_poverty', \n",
    "    's1903_median_income', 's2101_veteran_status', 's2201_food_stamps', 's2301_employment_status', \n",
    "    's2401_occupation_sex', 's2403_industry_sex', 's2501_occupancy_characteristics', \n",
    "    's2503_financial_characteristics', 's2701_health_insurance',\n",
    "]\n",
    "\n",
    "active_target = 'partisanship_change'\n",
    "all_targets = ['partisanship', 'partisanship_change', 'partisanship_change_amount', 'turnout_pct']\n",
    "\n",
    "# Rank n top features: [1, < infinity]\n",
    "top_n_features_to_rank = 100000000\n",
    "\n",
    "# Predict with n top features\n",
    "# 16 = 0.4697 w/o engineered features\n",
    "# 23 = 16 + 7 engineered fields we later throw away\n",
    "top_n_feature_for_preds = 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08636b5-bdc3-49dc-b8c2-57e3092fcde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in YEARS:\n",
    "    print(f'Processing year {year}')\n",
    "    \n",
    "    for office in OFFICES:\n",
    "        print(f'Processing office {office}')\n",
    "        \n",
    "        df_precincts = pd.read_csv('data/generated_data/df_06_tract_' + year + '_' + office.replace('.', '').replace(' ', '_') + '.csv')\n",
    "        df_precincts['standardized_id_num'] = df_precincts['standardized_id_num'].astype(str).str.zfill(13)\n",
    "        \n",
    "        df_precincts[\"partisanship\"] = df_precincts.apply(categorize_partisanship, axis=1)\n",
    "        df_precincts[\"partisanship_change\"] = df_precincts.apply(lambda row: categorize_partisan_change(row), axis=1) # find the change\n",
    "        df_precincts[\"partisanship_change_amount\"] = df_precincts.apply(lambda row: categorize_partisan_change_amount(row), axis=1) # measure how big the change was\n",
    "\n",
    "        print(f'Loading census data')\n",
    "        census_dataset_dfs = []\n",
    "        for census_dataset in census_datasets:\n",
    "            census_dataset = census_dataset.lower()\n",
    "            if census_dataset[:1] == 's':\n",
    "                census_dataset_code = census_dataset[:5].upper()\n",
    "                census_dataset_label = census_dataset[6:]\n",
    "            elif census_dataset[:1] == 'b':\n",
    "                census_dataset_code = census_dataset[:6].upper()\n",
    "                census_dataset_label = census_dataset[7:]\n",
    "            \n",
    "            df_census_dataset = pd.read_csv(f'data/generated_data/df_06_{census_dataset_label}_' + year + '_' + office.replace('.', '').replace(' ', '_') + '.csv')\n",
    "            df_census_dataset.rename(columns={f'geoid_{census_dataset_label}': 'geoidfq_tract'}, inplace=True)\n",
    "            \n",
    "            census_dataset_dfs.append(df_census_dataset)\n",
    "        \n",
    "        print(f'Merging precinct and census data')\n",
    "        \n",
    "        dfs = [df_precincts]\n",
    "        dfs.extend(census_dataset_dfs)\n",
    "        \n",
    "        df = reduce(lambda left, right: pd.merge(left, right, on='geoidfq_tract', how='left'), dfs)\n",
    "        df['standardized_id_num'] = df['standardized_id_num'].astype(str).str.zfill(13)\n",
    "        \n",
    "        print(f'Cleaning columns')\n",
    "        \n",
    "        drop_cols = [\n",
    "            \"City/Township Description\", \"District Code\", \"Election Type\", # would add noise\n",
    "            \"Michigan County Code\", # Prefer census code, and would need one-hot-encoding with sparsity\n",
    "            \"Office Description\", \"Precinct Label\", \"Status Code\",  # would add noise\n",
    "            \"County Name\", \"Precinct Number\", \"Election Year\",  # would add noise\n",
    "            \"total_votes\", \"registered_voters\", \"turnout_pct\", # don't look into the future\n",
    "            \"nearest_bound_census_tract\", \"nearest_bound_school_district\", \"nearest_bound_zipcode\",  # would add noise\n",
    "            \"Office Code\", \"Census County Code\", # Would need one-hot-encoding, not too much sparsity though\n",
    "            \"City/Township Code\", \"Ward Number\",  # would add noise\n",
    "            \"county\", \"office\", \"electionye\", # addressed elsewhere\n",
    "            \"registered_voters_change\", # raw population varies too widely\n",
    "            \"dem_votes\", \"rep_votes\", \"oth_votes\", # don't look into the future\n",
    "            \"dem_share\", \"rep_share\", \"oth_share\", # don't look into the future\n",
    "            \"dem_votes_change\", \"rep_votes_change\", \"oth_votes_change\", # raw population varies too widely\n",
    "            \"locale_full\", \"objectid\", \"precinct_num\", \"precinct_wp_id\", # would add noise\n",
    "            \"standardized_id\", \"geometry\", \"geometry_tract\", \"geoidfq_tract\", \"name_tract\", # would add noise\n",
    "            \"subdivision_fips\", \"ward_num\", \"locale_full\", \"county_fips\", \"objectid\", \"precinct_num\", # would add noise\n",
    "            \"nearest_tract\", \"awater_tract\", \"aland_tract\", \"tractce_tract\", \"shapestarea\", \"shapestlength\", \"geoid_tract\", # try not using tract geo characteristics\n",
    "        ]\n",
    "        \n",
    "        string_columns = [\"standardized_id_num\", \"partisanship\", \"partisanship_change\"]\n",
    "        df = df.drop(columns=drop_cols, errors='ignore')\n",
    "        numeric_df = df.drop(columns=string_columns, errors='ignore')\n",
    "        string_df = df[string_columns]\n",
    "        numeric_df = numeric_df.apply(pd.to_numeric, errors='coerce')\n",
    "        numeric_df = numeric_df.fillna(numeric_df.median())\n",
    "        df = pd.concat([string_df, numeric_df], axis=1)\n",
    "        \n",
    "        df['standardized_id_num'] = df['standardized_id_num'].astype(str).str.zfill(13)\n",
    "        df = df.dropna(subset=['partisanship_change', 'partisanship_change_amount'])\n",
    "        \n",
    "        # Define inactive targets by removing active_target from all_targets\n",
    "        excluded_targets = [item for item in all_targets if item != active_target]\n",
    "\n",
    "        # Exclude any other features.\n",
    "        excluded_features = [\n",
    "            \"standardized_id_num\", \"registered_voters\",\n",
    "            # \"dem_share_prev\", \"rep_share_prev\", \"oth_share_prev\",  # Huge predictor\n",
    "            # \"dem_share_change\", \"rep_share_change\", \"oth_share_change\",  # Huge predictor\n",
    "            # \"turnout_pct_change\",\n",
    "        ]\n",
    "        excluded_features.extend(excluded_targets)\n",
    "        \n",
    "        # Encode target and *potential* categorical features. \n",
    "        label_encoder = LabelEncoder()\n",
    "        cat_cols = [active_target]\n",
    "        for col in cat_cols:\n",
    "            df[col] = df[col].astype(str)\n",
    "            df[col] = label_encoder.fit_transform(df[col])\n",
    "        \n",
    "        X = df.drop(columns=[active_target] + excluded_features, errors='ignore')\n",
    "        y = df[active_target]\n",
    "\n",
    "        # Impute, scale, and split\n",
    "        print(f'Imputing missing features')\n",
    "        imputer = SimpleImputer(strategy=\"median\", add_indicator=True)\n",
    "        X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "        X_imputed = imputer.fit_transform(X)\n",
    "        new_column_names = list(X.columns) + [f\"missing_{i}\" for i in range(X_imputed.shape[1] - X.shape[1])]\n",
    "        X = pd.DataFrame(X_imputed, columns=new_column_names)\n",
    "        y = y.fillna(y.median())\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        X = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        print(f'Measuring feature importance')\n",
    "        \n",
    "        # Correlation\n",
    "        correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "        \n",
    "        # Random Forest\n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        rf_importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "        \n",
    "        # Lasso\n",
    "        lasso = LassoCV(cv=5, random_state=42).fit(X_train, y_train)\n",
    "        lasso_importances = pd.Series(np.abs(lasso.coef_), index=X.columns).sort_values(ascending=False)\n",
    "        \n",
    "        # Mutual info\n",
    "        mi_scores = mutual_info_regression(X_train, y_train)\n",
    "        mi_importances = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "        \n",
    "        # SHAP Values via XGBoost\n",
    "        xgb_model = xgb.XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "        shap_sample = X_train.sample(n=min(500, len(X_train)), random_state=42) # Use only 500 rows for X_train\n",
    "        explainer = shap.Explainer(xgb_model, shap_sample)\n",
    "        shap_values = explainer(shap_sample)\n",
    "        shap_importances = pd.Series(np.abs(shap_values.values).mean(axis=0), index=X.columns).sort_values(ascending=False)\n",
    "        \n",
    "        # Feature importance rankings\n",
    "        feature_rankings = pd.DataFrame({\n",
    "            \"Correlation\": correlations,\n",
    "            \"RandomForest\": rf_importances,\n",
    "            \"Lasso\": lasso_importances,\n",
    "            \"MutualInfo\": mi_importances,\n",
    "            \"SHAP\": shap_importances\n",
    "        })\n",
    "        feature_rankings_ranked = feature_rankings.rank(axis=0, pct=True, method=\"average\", ascending=False)\n",
    "        feature_rankings[\"Average_Rank\"] = feature_rankings_ranked.mean(axis=1)\n",
    "        feature_rankings['Feature name'] = feature_rankings.index[:top_n_features_to_rank]\n",
    "        feature_rankings.sort_values('Average_Rank', ascending=True)\n",
    "        feature_rankings.to_csv(f'data/generated_data/feature_rankings_{year}_{office.replace('.', '').replace(' ', '_')}.csv', index=None)\n",
    "        feature_rankings.drop(columns=['Feature name'], inplace=True)\n",
    "\n",
    "        # Feature importance histograms\n",
    "        print(f'Plotting feature importances')\n",
    "        ranking_types = [\"Average_Rank\", \"Correlation\", \"RandomForest\", \"Lasso\", \"MutualInfo\", \"SHAP\"]        \n",
    "        for ranking_type in ranking_types:\n",
    "            features = feature_rankings.sort_values(ranking_type, ascending=False)\n",
    "            plt.figure(figsize=(60, 140))\n",
    "            plt.subplots_adjust(left=0.35)\n",
    "            sns.barplot(x=features[ranking_type] / features[ranking_type].max(), y=features.index, orient=\"h\")\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.ylabel(\"Feature\")\n",
    "            plt.xlabel(ranking_type)\n",
    "            plt.title(f\"Top Features for Predicting {active_target}\")\n",
    "            plt.savefig(f'output/figures/Features_Histogram_{ranking_type}_{year}_{office}.png')\n",
    "            plt.close()\n",
    "        \n",
    "        # Feature correlation heatmap\n",
    "        top_features = feature_rankings.index[:top_n_features_to_rank]\n",
    "        corr_matrix = X[top_features].corr()\n",
    "        # np.fill_diagonal(corr_matrix.values, 0)\n",
    "        \n",
    "        plt.figure(figsize=(96, 80))\n",
    "        plt.subplots_adjust(left=0.35)\n",
    "        plt.subplots_adjust(bottom=0.35)\n",
    "        sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5, square=True)\n",
    "        plt.title(f\"Correlation Heatmap of Top {top_n_features_to_rank} Features for Predicting {active_target}\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.savefig(f'output/figures/Features_Correlation_{year}_{office}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        '''\n",
    "        Individual Feature Predictive Power (Univariate Performance)\n",
    "        Evaluate each feature’s ability to predict the target variable alone, using:\n",
    "        \n",
    "            Univariate Regression for Continuous Targets\n",
    "                Train a simple regression model (LinearRegression or DecisionTreeRegressor) on each feature separately.\n",
    "                Measure R² (coefficient of determination) for how well each feature alone explains variation in y.\n",
    "        \n",
    "            Univariate Classification for Categorical Targets\n",
    "                Train a simple classifier (DecisionTreeClassifier or LogisticRegression) using only one feature at a time.\n",
    "                Measure accuracy, AUC (for ordinal relationships), or F1-score.\n",
    "        '''\n",
    "        print(f'Ranking feature importance')\n",
    "        feature_performance = []\n",
    "        is_continuous = y.dtype.kind in 'fc' # float or continuous\n",
    "        for feature in X.columns:\n",
    "            X_feature = X[[feature]]\n",
    "            X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(X_feature, y, test_size=0.2, random_state=42)\n",
    "            if is_continuous:  # Regression\n",
    "                model = LinearRegression()\n",
    "                model.fit(X_train_feat, y_train_feat)\n",
    "                y_pred = model.predict(X_test_feat)\n",
    "                score = r2_score(y_test_feat, y_pred)  # R² for regression\n",
    "                metric = \"R² Score\"\n",
    "            else:  # Classification\n",
    "                model = LogisticRegression(max_iter=200)  # or DecisionTreeClassifier() if needed\n",
    "                model.fit(X_train_feat, y_train_feat)\n",
    "                y_pred = model.predict(X_test_feat)\n",
    "                score = accuracy_score(y_test_feat, y_pred)  # Accuracy for classification\n",
    "                metric = \"Accuracy\"\n",
    "            feature_performance.append({\"Feature\": feature, metric: score})\n",
    "        feature_performance_df = pd.DataFrame(feature_performance).sort_values(by=metric, ascending=False)\n",
    "        feature_performance_df.to_csv(f'data/generated_data/feature_performance_{year}_{office.replace('.', '').replace(' ', '_')}.csv', index=None)\n",
    "\n",
    "        '''\n",
    "        Drop-Column Feature Importance (Permutation Importance)\n",
    "            Train a full model with all features.\n",
    "            Then remove one feature at a time, retrain the model, and measure the drop in accuracy.\n",
    "            A bigger accuracy drop means that feature is highly predictive.\n",
    "        '''\n",
    "        print(f'Measuring permutation (random forest regr.) performance')\n",
    "        model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        perm_importance = permutation_importance(model, X_test, y_test, scoring=\"r2\")\n",
    "        perm_importance_df = pd.DataFrame({\n",
    "            \"Feature\": X.columns,\n",
    "            \"Importance\": perm_importance.importances_mean\n",
    "        }).sort_values(by=\"Importance\", ascending=False)\n",
    "        perm_importance_df.to_csv(f'data/generated_data/permutation_important_{year}_{office.replace('.', '').replace(' ', '_')}.csv', index=None)\n",
    "\n",
    "        '''\n",
    "        SHAP Values for Predictive Power\n",
    "            SHAP values tell how much each feature influences a model’s predictions on average.\n",
    "            Higher SHAP values mean a feature contributes strongly to predictions.\n",
    "        '''\n",
    "        print(f'Measuring SHAP performance')\n",
    "        model = xgb.XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        explainer = shap.Explainer(model, X_test)\n",
    "        shap_values = explainer(X_test)\n",
    "        shap_importance = pd.DataFrame({\n",
    "            \"Feature\": X.columns,\n",
    "            \"SHAP Importance\": np.abs(shap_values.values).mean(axis=0)\n",
    "        }).sort_values(by=\"SHAP Importance\", ascending=False)\n",
    "        shap_importance.to_csv(f'data/generated_data/shap_importance_{year}_{office.replace('.', '').replace(' ', '_')}.csv', index=None)\n",
    "\n",
    "\n",
    "        ################################################\n",
    "        # MAKE THE ACTUAL PREDICTIONS\n",
    "        ################################################\n",
    "        print(f'Create the predictor')\n",
    "        \n",
    "        # Random Forest\n",
    "        print(f'Train random forest model')\n",
    "        best_features = rf_importances.index[:top_n_feature_for_preds].tolist()\n",
    "        valid_features = [feat for feat in best_features if feat in X.columns]\n",
    "        X_selected = X[valid_features]\n",
    "        y_encoded = df[active_target]\n",
    "\n",
    "        # Target column is no longer encoded (unseen labels error)\n",
    "        assert np.issubdtype(df[active_target].dtype, np.integer)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_selected, y_encoded, test_size=0.2, random_state=42)\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict\n",
    "        print(f'Make predictions')\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Re-convert labels\n",
    "        all_labels = list(range(len(label_encoder.classes_)))\n",
    "        target_names = list(label_encoder.classes_)\n",
    "\n",
    "        print(f'Evaluate predictions')\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, labels=all_labels, target_names=target_names))\n",
    "        \n",
    "        # Predict each precinct\n",
    "        df[f\"predicted_{active_target}\"] = label_encoder.inverse_transform(model.predict(X_selected))\n",
    "        \n",
    "        # Reconvert labels\n",
    "        # df[active_target] = label_encoder.inverse_transform(df[active_target])\n",
    "        df[f\"{active_target}_decoded\"] = label_encoder.inverse_transform(df[active_target])\n",
    "\n",
    "        # Full-dataset accuracy like Excel\n",
    "        y_all_true = df[active_target]\n",
    "        y_all_pred = df[f\"predicted_{active_target}\"]\n",
    "        overall_accuracy = np.mean(y_all_true == y_all_pred)\n",
    "        print(f\"Full Dataset Accuracy (Excel-style): {overall_accuracy:.4f}\")\n",
    "        \n",
    "        # Get precinct id, ground truth, and prediction\n",
    "        df = df[[\"standardized_id_num\", active_target, f\"predicted_{active_target}\"]]\n",
    "        df[\"standardized_id_num\"] = df[\"standardized_id_num\"].apply(lambda x: str(x).replace('.0', '').zfill(13))\n",
    "        df.to_csv(f\"data/generated_data/predicted_{active_target}_{year}_{office.replace('.', '').replace(' ', '_')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888a5e2-b825-4d6d-8fea-e050614e8c95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
